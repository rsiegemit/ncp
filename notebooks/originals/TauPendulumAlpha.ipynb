{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9048,"status":"ok","timestamp":1750702319120,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"9UHHV6NNPhxA"},"outputs":[],"source":["import torch\n","import os\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","sentinel = 1337"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1750702319123,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"OZ0JZNr1SSqA"},"outputs":[],"source":["def search_alpha_parallel(sol, V_start, radius, t_eval, L):\n","\n","    if sol.dim() > 1:\n","      sol = sol[:, 1:]\n","    else:\n","      sol = sol[1:]\n","    t_eval = t_eval[1:]\n","    ceiling = 1e5\n","    alpha_h = torch.tensor([1], device = device)*torch.ones_like(V_start)\n","    alpha_l = torch.tensor([-1], device = device)*torch.ones_like(V_start)\n","    if sol.dim() > 1:\n","      r = radius.unsqueeze(1).expand(sol.size())\n","    else:\n","      r = radius.unsqueeze(0).expand(sol.size())\n","\n","    if True:\n","      condition_h = torch.min(sol*torch.exp(torch.outer(alpha_h,t_eval)) + torch.mul(r, torch.exp(torch.outer(L + alpha_h,t_eval))), dim = 1).values > V_start - radius\n","      condition_l = torch.min(sol*torch.exp(torch.outer(alpha_l,t_eval)) + torch.mul(r, torch.exp(torch.outer(L + alpha_l,t_eval))), dim = 1).values < V_start - radius\n","\n","\n","    while torch.any(~condition_h) or torch.any(~condition_l):\n","      alpha_h[~condition_h] = alpha_h[~condition_h]*2\n","      alpha_l[~condition_l] = alpha_l[~condition_l]*2\n","      if True:\n","        #print('uh oh')\n","        condition_h = torch.min(sol*torch.exp(torch.outer(alpha_h,t_eval)) + torch.mul(r, torch.exp(torch.outer(L + alpha_h,t_eval))), dim = 1).values > V_start - radius\n","        condition_l = torch.min(sol*torch.exp(torch.outer(alpha_l,t_eval)) + torch.mul(r, torch.exp(torch.outer(L + alpha_l,t_eval))), dim = 1).values < V_start - radius\n","\n","    alpha_mid = (alpha_h+alpha_l)/2\n","    while torch.max(torch.abs(alpha_h-alpha_l))>0.0001:\n","        if True:\n","          condition = torch.min(sol*torch.exp(torch.outer(alpha_mid,t_eval)) + torch.mul(r, torch.exp(torch.outer(L + alpha_mid,t_eval))), dim = 1).values < V_start - radius\n","\n","        alpha_l[condition] = alpha_mid[condition]\n","        alpha_h[~condition] = alpha_mid[~condition]\n","        alpha_mid = (alpha_h+alpha_l)/2\n","\n","    indices = torch.argmin(sol*torch.exp(torch.outer(alpha_mid,t_eval)) + torch.mul(r, torch.exp(torch.outer(L + alpha_mid,t_eval))), dim = 1)\n","    return alpha_mid, indices"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1750702319125,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"8qVu14bKZIKr"},"outputs":[],"source":["def certify_alpha(sol, V_start, radius, t_eval, L, alpha):\n","    sol = sol[:, 1:]\n","    t_eval = t_eval[1:]\n","    alpha = alpha*torch.tensor([1], device = device)*torch.ones_like(V_start)\n","    r = radius.unsqueeze(1).expand(sol.size())\n","\n","    condition = torch.min(sol*torch.exp(torch.outer(alpha,t_eval)) + torch.mul(r, torch.exp(torch.outer(L + alpha,t_eval))), dim = 1).values < V_start - radius\n","\n","    indices = torch.argmin(sol*torch.exp(torch.outer(alpha,t_eval)) + torch.mul(r, torch.exp(torch.outer(L + alpha,t_eval))), dim = 1)\n","    return condition, indices"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1750702319136,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"t_DOS2gWEp_F"},"outputs":[],"source":["import torch\n","import math\n","\n","def wrap_to_pi(tensor):\n","    \"\"\"\n","    Wrap angles in the last-dim index 0 (i.e., theta) to (-pi, pi]\n","    for any tensor ending in dimension 2 (e.g., (m, n, 2)).\n","    \"\"\"\n","    wrapped = (tensor[..., 0] + math.pi) % (2 * math.pi) - math.pi\n","    result = tensor.clone()\n","    result[..., 0] = wrapped\n","    return result\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1750702319151,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"eVg7nBixPh9d"},"outputs":[],"source":["import torch\n","\n","def find_hypercubes_old(points, centers, radii):\n","    \"\"\"\n","    Vectorized version: returns the first matching hypercube index for each point,\n","    or -1 if none are found.\n","    \"\"\"\n","\n","    points = wrap_to_pi(points)\n","    k, n = points.shape\n","    m, _ = centers.shape\n","\n","    if radii.dim() == 1:\n","        radii = radii.unsqueeze(1)\n","    radii = radii.expand(m, n)\n","\n","    lower_bounds = centers - radii  # (m, n)\n","    upper_bounds = centers + radii  # (m, n)\n","\n","    # (k, 1, n) vs (1, m, n) → (k, m, n)\n","    points = points.unsqueeze(1)\n","    contained = (points >= lower_bounds) & (points <= upper_bounds)\n","    inside_mask = contained.all(dim=2)  # (k, m)\n","\n","    # Set all False to large positive index (m), then take min index along dim=1\n","    masked_indices = torch.where(inside_mask, torch.arange(m, device=points.device), m)\n","    min_indices = masked_indices.min(dim=1).values\n","\n","    # Set to -1 if no hypercube matched (i.e. if index == m)\n","    result = torch.where(min_indices == m, torch.full_like(min_indices, -1), min_indices)\n","\n","\n","    return result\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1750702319157,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"llj3g3nwLwlP"},"outputs":[],"source":["import torch\n","\n","def find_hypercubes(points, centers, radii):\n","    \"\"\"\n","    Vectorized function to find which square (if any) each point falls into.\n","\n","    Args:\n","        points: (k, 2) tensor of 2D points.\n","        centers: (m, 2) tensor of square centers.\n","        radii: (m,) tensor of square half-widths.\n","\n","    Returns:\n","        (k,) tensor of square indices, or -1 if not inside any square.\n","    \"\"\"\n","    k = points.shape[0]\n","    m = centers.shape[0]\n","\n","    # Expand for vectorized broadcasted comparison\n","    points_exp = points[:, None, :]        # (k, 1, 2)\n","    centers_exp = centers[None, :, :]      # (1, m, 2)\n","    radii_exp = radii[None, :, None]       # (1, m, 1)\n","\n","    lower = centers_exp - radii_exp        # (1, m, 2)\n","    upper = centers_exp + radii_exp        # (1, m, 2)\n","\n","    # (k, m, 2): whether each point is within each square in both dims\n","    contained = (points_exp >= lower) & (points_exp <= upper)\n","    contained = contained.all(dim=-1)      # (k, m): True if point i in square j\n","\n","    any_match = contained.any(dim=1)       # (k,)\n","    match_idx = torch.argmax(contained.int(), dim=1)  # (k,): first match\n","\n","    # Replace unmatched with -1\n","    result = torch.where(any_match, match_idx, torch.full_like(match_idx, -1))\n","\n","    return result\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1750702319189,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"H_Jm0xfCPrtP"},"outputs":[],"source":["import torch\n","\n","def inverted_pendulum_2d_torch(x, u, m=0.1, l=10, g=9.81):\n","    \"\"\"\n","    2D simplified inverted pendulum dynamics (batch version with PyTorch).\n","\n","    Args:\n","        x: Tensor of shape (n, 2) -> [theta, theta_dot]\n","        u: Tensor of shape (n,) or (n,1) -> control input\n","        m: Mass of pendulum\n","        l: Length to center of mass\n","        g: Gravitational acceleration\n","\n","    Returns:\n","        x_dot: Tensor of shape (n, 2) -> [theta_dot, theta_ddot]\n","    \"\"\"\n","    theta = x[:, 0]\n","    theta_dot = x[:, 1]\n","\n","    # Ensure u has shape (n,)\n","    u = u.squeeze(-1)\n","\n","    # Compute second derivative\n","    theta_ddot = (g / l) * torch.sin(theta) + (u / (m * l)) * torch.abs(torch.cos(theta))\n","\n","    # Pack into (n,2)\n","    x_dot = torch.stack([theta_dot, theta_ddot], dim=1)\n","\n","    return x_dot\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":176,"status":"ok","timestamp":1750702319366,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"-4vPTsSqTvZW"},"outputs":[],"source":["import torch\n","\n","def simplified_pendulum_derivatives(x, u):\n","    \"\"\"\n","    Pendulum-v1 dynamics exactly as implemented in OpenAI Gym.\n","\n","    Args:\n","        x: Tensor of shape (batch_size, 2) – [theta, theta_dot]\n","        u: Tensor of shape (batch_size,) – torque input in [-2, 2]\n","\n","    Returns:\n","        dxdt: Tensor of shape (batch_size, 2) – [dtheta, dtheta_dot]\n","    \"\"\"\n","    theta = x[:, 0]\n","    theta_dot = x[:, 1]\n","\n","    g = 10.0   # gravity (as used in Gym)\n","    torque_coeff = 15.0\n","\n","    dtheta = theta_dot\n","    dtheta_dot = 3 * g * torch.sin(theta) + torque_coeff * u\n","\n","    dxdt = torch.stack([dtheta, dtheta_dot], dim=1)\n","    return dxdt\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1750702319368,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"wqP56kIwjAQS"},"outputs":[],"source":["import torch\n","import math\n","\n","def count_hypercube_intersections_wrap(centers, radii, query_centers, query_radii):\n","    \"\"\"\n","    Count how many hypercubes intersect each query hypercube.\n","    Dimension 0 is angular and wraps around at 2π.\n","    \"\"\"\n","    N, D = centers.shape\n","    M = query_centers.shape[0]\n","\n","    if radii.ndim == 1:\n","        radii = radii.unsqueeze(1).expand(N, D)\n","    elif radii.shape[1] == 1:\n","        radii = radii.expand(N, D)\n","\n","    if query_radii.ndim == 1:\n","        query_radii = query_radii.unsqueeze(1).expand(M, D)\n","    elif query_radii.shape[1] == 1:\n","        query_radii = query_radii.expand(M, D)\n","\n","    # Expand for broadcasting\n","    centers = centers.unsqueeze(0)             # (1, N, D)\n","    radii = radii.unsqueeze(0)                 # (1, N, D)\n","    query_centers = query_centers.unsqueeze(1) # (M, 1, D)\n","    query_radii = query_radii.unsqueeze(1)     # (M, 1, D)\n","\n","    # Compute wrapped angular distance in dimension 0\n","    angle_diff = torch.abs(centers[..., 0] - query_centers[..., 0])  # (M, N)\n","    angle_dist = torch.minimum(angle_diff, 2 * math.pi - angle_diff)  # (M, N)\n","\n","    # Compute regular distance for other dimensions\n","    if D > 1:\n","        euclidean_dist = torch.abs(centers[..., 1:] - query_centers[..., 1:])  # (M, N, D-1)\n","        euclidean_thresh = radii[..., 1:] + query_radii[..., 1:]               # (M, N, D-1)\n","        euclidean_mask = euclidean_dist <= euclidean_thresh                   # (M, N, D-1)\n","        angular_thresh = radii[..., 0] + query_radii[..., 0]                  # (M, N)\n","        angular_mask = angle_dist <= angular_thresh                           # (M, N)\n","\n","        # Combine angular and Euclidean masks\n","        intersects = angular_mask & euclidean_mask.all(dim=-1)               # (M, N)\n","    else:\n","        # Only one dimension (angular case)\n","        angular_thresh = radii[..., 0] + query_radii[..., 0]\n","        intersects = angle_dist <= angular_thresh\n","\n","    return intersects.sum(dim=1)  # (M,)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":185,"status":"ok","timestamp":1750702319554,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"N4YvxTpnjAkV"},"outputs":[],"source":["import torch\n","import math\n","\n","def find_hypercube_intersections_wrap(centers, radii, query_centers, query_radii):\n","    \"\"\"\n","    For each query hypercube, return a list of tensors with the indices of intersecting hypercubes.\n","    The first dimension (dimension 0) is angular and wraps at 2π.\n","    \"\"\"\n","    N, D = centers.shape\n","    M = query_centers.shape[0]\n","\n","    # Expand radii to (N, D)\n","    if radii.ndim == 1:\n","        radii = radii.unsqueeze(1).expand(N, D)\n","    elif radii.shape[1] == 1:\n","        radii = radii.expand(N, D)\n","\n","    if query_radii.ndim == 1:\n","        query_radii = query_radii.unsqueeze(1).expand(M, D)\n","    elif query_radii.shape[1] == 1:\n","        query_radii = query_radii.expand(M, D)\n","\n","    # Expand for broadcasting\n","    qc = query_centers.unsqueeze(1)  # (M, 1, D)\n","    qr = query_radii.unsqueeze(1)    # (M, 1, D)\n","    c = centers.unsqueeze(0)         # (1, N, D)\n","    r = radii.unsqueeze(0)           # (1, N, D)\n","\n","    # Wrapped distance in angular dimension (dim 0)\n","    angle_diff = torch.abs(c[..., 0] - qc[..., 0])                 # (M, N)\n","    angle_dist = torch.minimum(angle_diff, 2 * math.pi - angle_diff)  # (M, N)\n","    angle_thresh = r[..., 0] + qr[..., 0]                          # (M, N)\n","    angle_mask = angle_dist < angle_thresh                        # (M, N)\n","\n","    if D > 1:\n","        # Regular distance in remaining dimensions\n","        dist = torch.abs(c[..., 1:] - qc[..., 1:])                 # (M, N, D-1)\n","        threshold = r[..., 1:] + qr[..., 1:]                       # (M, N, D-1)\n","        euclidean_mask = dist < threshold                        # (M, N, D-1)\n","        total_mask = angle_mask & euclidean_mask.all(dim=-1)      # (M, N)\n","    else:\n","        total_mask = angle_mask  # only 1D\n","\n","    # Get (query_idx, box_idx) pairs\n","    query_idx, box_idx = torch.nonzero(total_mask, as_tuple=True)\n","\n","    # Group indices by query\n","    counts = torch.bincount(query_idx, minlength=M)\n","    splits = counts.cumsum(0)\n","    splits = torch.cat([splits.new_zeros(1), splits])\n","\n","    return [box_idx[splits[i]:splits[i+1]] for i in range(M)]\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1750702319556,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"nbh6dLJ8P3vE"},"outputs":[],"source":["def check_parallel(batch_states, t, r, alpha,\n","                               L, rate, num_samples,\n","                               unverified_centers, unverified_radii,\n","                               verified_centers, verified_radii,\n","                               verified_alphas, verified_indices, trajectories):\n","    device = batch_states.device\n","    B = batch_states.shape[0]\n","\n","    # Scale radius\n","    scaled_r = r * np.exp(L * t * rate)\n","    radii_tensor = scaled_r * torch.ones(B, device=device)\n","\n","    # Count unverified intersections\n","    unverified_count = count_hypercube_intersections_wrap(\n","         unverified_centers, unverified_radii, batch_states, radii_tensor\n","    )\n","    eligible_mask = unverified_count == 0\n","    eligible_indices = torch.nonzero(eligible_mask, as_tuple=False).squeeze(1)\n","\n","    # Filter current candidates\n","    query_states = batch_states[eligible_indices]\n","    query_radii = radii_tensor[eligible_indices]\n","\n","\n","    # Get verified intersections\n","    verified_lists = find_hypercube_intersections_wrap(\n","       verified_centers, verified_radii, query_states, query_radii,\n","    )\n","\n","    # Flatten into (K_total,) for all (k, k_i) pairs\n","    k_offsets = torch.arange(len(verified_lists), device=device)\n","    row_counts = torch.tensor([v.numel() for v in verified_lists], device=device)\n","    if row_counts.sum() == 0:\n","        return eligible_indices.new_empty(0)\n","\n","    k_idx = torch.repeat_interleave(k_offsets, row_counts)\n","    k_i_idx = torch.cat(verified_lists, dim=0)\n","\n","\n","    # Gather data\n","    q_states = query_states[k_idx]                         # (K, D)\n","    q_radii = query_radii[k_idx]                           # (K,)\n","\n","    v_centers = verified_centers[k_i_idx]                  # (K, D)\n","    v_radii = verified_radii[k_i_idx]                      # (K,)\n","    v_alphas = verified_alphas[k_i_idx]                    # (K,)\n","    v_indices = verified_indices[k_i_idx]                  # (K,)\n","\n","    traj_k = trajectories[eligible_indices[k_idx], :, 0]   # (K, T+1)\n","\n","    norm_traj = torch.norm(traj_k, dim=1) - r[eligible_indices][k_idx]         # (K,)\n","    term1 = torch.exp(-(v_alphas - alpha) * (v_indices) * rate)\n","    term2 = torch.exp(v_alphas * t * rate)\n","    numerator = (v_centers + v_radii.unsqueeze(1)).norm(dim=1)\n","    inequality = term1 * term2 * numerator / norm_traj     # (K,)\n","\n","    # Per k: check if all inequality ≤ 1\n","    failed = inequality > 1\n","    failed_mask = torch.zeros(len(verified_lists), dtype=torch.bool, device=device)\n","    failed_mask.index_put_((k_idx[failed],), torch.ones_like(k_idx[failed], dtype=torch.bool), accumulate=True)\n","\n","    # Final: only those k where all passed\n","    passed_mask = ~failed_mask\n","    return eligible_indices[passed_mask]\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":62,"status":"ok","timestamp":1750702319616,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"f2YVlz-ME0CS"},"outputs":[],"source":["from typing_extensions import final\n","import torch\n","import numpy as np\n","import itertools\n","import gc\n","\n","class PendulumEnv:\n","    def __init__(self,\n","                 num_envs=10,    # Number of parallel environments\n","                 target = None,   # Allow custom target location\n","                 max_radius=5.0,         # Define boundary size\n","                 max_speed=1.0,           # Limit agent's velocity\n","                 precision = 0.03,        # Define the allowed distance from the target\n","                 step_penalization = 2,   # How strongly each step taken is penalized\n","                 rate = 0.1,              # Rate of movement\n","                 field_function = inverted_pendulum_2d_torch,  # Allow different field functions\n","                 reward_type=\"distance\",   # Allow different reward strategies\n","                 dimension = 2,\n","                 max_starting_speed = 1.0,\n","                 min_alpha = 0.01,\n","                 L = 5,\n","                 eval_func = False\n","                ):\n","\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.num_envs = num_envs\n","\n","        self.max_speed = max_speed\n","        self.max_radius = max_radius\n","        self.precision = precision\n","        self.step_penalization = step_penalization\n","        self.rate = rate\n","        self.field_function = field_function\n","        self.reward_type = reward_type\n","        self.dim = dimension\n","        self.f = field_function\n","        self.max_starting_speed = max_starting_speed\n","        self.alpha = min_alpha\n","        self.L = L\n","        self.eval_func = eval_func\n","\n","        if target == None:\n","          self.target = torch.zeros(self.dim, device = self.device)\n","        else:\n","          self.target = torch.tensor(target, device=self.device, dtype=torch.float32)\n","\n","\n","        # Initialize states (x, y, vx, vy) for all environments in parallel\n","        self.states = self.sample_points_in_circle(self.num_envs).to(self.device)\n","        self.states = self.sample_points_in_hypercube(self.num_envs).to(self.device)\n","\n","    def sample_points_in_circle(self, batch_size):\n","        \"\"\" Sample random starting points within a circle of max_radius. \"\"\"\n","\n","        # Step 1: Sample from normal distribution (for uniform direction)\n","        points = torch.randn((batch_size, self.dim), device=self.device)\n","\n","        # Step 2: Normalize to lie on the unit n-sphere\n","        points /= torch.norm(points, dim=1, keepdim=True)\n","\n","        # Step 3: Sample radii with correct volume scaling\n","        radii = torch.rand((batch_size, 1), device=self.device) ** (1 / self.dim)  # Ensure uniform density\n","\n","        # Step 4: Scale points by radii\n","        return self.max_radius * points * radii\n","\n","    def sample_points(self, batch_size):\n","        \"\"\" Sample random starting points. \"\"\"\n","\n","        # Sample each coordinate independently from U(-1, 1)\n","        points = (torch.rand((batch_size, self.dim), device=self.device) * 2 - 1)  # Rescale [0,1] to [-1,1]\n","\n","        points = points * torch.tensor([self.max_radius, self.max_starting_speed], device=self.device)\n","\n","        return points\n","\n","    def sample_points_in_hypercube(self, batch_size):\n","        \"\"\" Sample random starting points uniformly within a hypercube. \"\"\"\n","\n","        # Sample each coordinate independently from U(-1, 1)\n","        points = (torch.rand((batch_size, self.dim), device=self.device) * 2 - 1)  # Rescale [0,1] to [-1,1]\n","\n","        # Scale by max_radius\n","        return self.max_radius * points\n","\n","\n","    def reset(self, seed = None):\n","        \"\"\" Reset all environments in parallel. \"\"\"\n","        if seed is not None:\n","          self.states = seed\n","        else:\n","          self.states = self.sample_points_in_hypercube(self.num_envs)\n","\n","\n","    def step(self, actions):\n","        \"\"\" Step function that updates all environments in parallel. \"\"\"\n","        actions = torch.clamp(actions, -self.max_speed, self.max_speed)\n","\n","        movement = self.f(self.states, actions)\n","        self.states += movement * self.rate  # Update positions\n","\n","        distances = torch.linalg.norm(self.states - self.target, dim=1, ord = float('inf'))\n","        rewards = -distances - self.step_penalization * self.rate\n","\n","        # Out-of-bounds penalty\n","        out_of_bounds = distances > self.max_radius\n","        rewards[out_of_bounds] = -10000\n","\n","        # Check if agents have reached the target\n","        done = distances < self.precision\n","        rewards[done] += 10000  # Bonus for reaching target\n","\n","        stop = torch.logical_or(done,out_of_bounds)\n","\n","        return self.states.clone(), rewards, stop\n","\n","    def sample_trajectory(self, time_steps=5, control_seed = None, variance: int = 1, num_samples: int = 1, r = None):\n","        \"\"\" Generate batch trajectories for multiple sampled controls and select the best one.\n","\n","        - If `control_seed` is provided, it should have shape (num_envs, time_steps, 2).\n","        - Otherwise, random controls are sampled from a normal distribution.\n","\n","        Returns:\n","        - Best control sequence (num_envs, time_steps, 2) -> **Direct input for `trajectories`**\n","        - Best trajectory per environment (num_envs, time_steps+1, 2)\n","        - Minimum final distance per environment (num_envs,)\n","        \"\"\"\n","\n","        variance = self.max_speed/3\n","        # Initialize batch states (only positions)\n","        t_eval = torch.linspace(0.0, time_steps*self.rate, steps=time_steps, device = device)\n","        if r == None:\n","          r = torch.zeros_like(t_eval)\n","        # Initialize batch states (only positions)\n","        batch_states = self.states.clone().repeat_interleave(num_samples, dim = 0)  # Shape: (num_envs * num_samples, 2)\n","\n","\n","\n","        # Handle control seeding\n","        if control_seed is not None:\n","            assert control_seed.shape == (self.num_envs, time_steps), \\\n","                \"control_seed must have shape (num_envs, time_steps, 2)\"\n","            mu = control_seed.unsqueeze(1).repeat(1, num_samples, 1)  # Expand for num_samples\n","        else:\n","            mu = torch.zeros((self.num_envs, num_samples, time_steps), device=self.device)\n","\n","        sigma = torch.ones_like(mu) * variance  # Add variance\n","        action_lists = torch.normal(mu, sigma)  # Sampled action sequences\n","\n","        action_lists = action_lists.view(-1, time_steps)  # Reshape to (num_envs * num_samples, time_steps)\n","        action_lists = torch.clamp(action_lists, -self.max_speed, self.max_speed)\n","\n","        # Track all trajectories\n","        trajectories = [batch_states.clone().unsqueeze(1)]  # Initial positions\n","        for t in range(time_steps):\n","            actions = action_lists[:, t]\n","            movement = self.f(batch_states, actions)\n","            batch_states = batch_states + movement * self.rate  # Move only by instantaneous action\n","            trajectories.append(batch_states.clone().unsqueeze(1))  # Store positions\n","\n","\n","\n","        # Stack to get (num_envs * num_samples, time_steps+1, 2)\n","        trajectories = torch.cat(trajectories, dim=1)\n","\n","        trajectories = wrap_to_pi(trajectories)\n","        batch_states = wrap_to_pi(batch_states)\n","\n","        # Compute final distances for selection\n","        final_positions = batch_states\n","\n","        if self.eval_func:\n","          dist = torch.sqrt(((trajectories - self.target)**2 * torch.tensor([1.0, 0.01], device=device)).sum(dim=2))[:, 1:]\n","        else:\n","          dist = torch.linalg.norm(trajectories - self.target, dim=2, ord = float('inf'))[:, 1:]\n","\n","        r = torch.repeat_interleave(r, num_samples)\n","        firstpart = dist*torch.exp(self.alpha*t_eval)\n","        secondpart = torch.mul(r.unsqueeze(1), torch.exp(((self.L + self.alpha)*t_eval)).unsqueeze(0))\n","        alphadist = firstpart + secondpart\n","\n","        #final_distances = torch.linalg.norm(final_positions - self.target, dim=1, ord = float('inf'))\n","        alpha_distances = torch.min(alphadist, dim = 1).values\n","        taus = torch.argmin(alphadist, dim = 1)\n","\n","        # Reshape distances to (num_envs, num_samples)\n","        alpha_distances = alpha_distances.view(self.num_envs, num_samples)\n","        taus = taus.view(self.num_envs, num_samples)\n","\n","        # Find the best trajectory for each environment\n","        best_indices = torch.argmin(alpha_distances, dim=1)  # Best sample for each environment\n","        taus = taus[torch.arange(self.num_envs), best_indices]\n","\n","\n","        # Gather best actions and trajectories\n","        best_actions = action_lists.view(self.num_envs, num_samples, time_steps)[torch.arange(self.num_envs), best_indices]\n","        best_trajectories = trajectories.view(self.num_envs, num_samples, time_steps+1, self.dim)[torch.arange(self.num_envs), best_indices]\n","\n","        return best_actions, best_trajectories, alpha_distances.min(dim=1).values, taus\n","\n","    def sample_trajectory_reuse(self, time_steps=5, control_seed = None, variance: int = 1, num_samples: int = 1, r = None, centers = None, radii = None, verified_controls = None, verified_indices = None, splits = None):\n","        \"\"\" Generate batch trajectories for multiple sampled controls and select the best one.\n","\n","        - If `control_seed` is provided, it should have shape (num_envs, time_steps, 2).\n","        - Otherwise, random controls are sampled from a normal distribution.\n","\n","        Returns:\n","        - Best control sequence (num_envs, time_steps, 2) -> **Direct input for `trajectories`**\n","        - Best trajectory per environment (num_envs, time_steps+1, 2)\n","        - Minimum final distance per environment (num_envs,)\n","        \"\"\"\n","\n","        #variance = self.max_speed/3\n","        # Initialize batch states (only positions)\n","        t_eval = torch.linspace(0.0, time_steps*self.rate, steps=time_steps, device = device)\n","        if r == None:\n","          r = torch.zeros_like(t_eval)\n","        # Initialize batch states (only positions)\n","        batch_states = self.states.clone().repeat_interleave(num_samples, dim = 0)  # Shape: (num_envs * num_samples, 2)\n","        if splits is not None:\n","          has_switched = (splits < 5).repeat_interleave(num_samples, dim = 0)\n","\n","\n","        # Handle control seeding\n","        if control_seed is not None:\n","            assert control_seed.shape == (self.num_envs, time_steps), \\\n","                \"control_seed must have shape (num_envs, time_steps, 2)\"\n","            mu = control_seed.unsqueeze(1).repeat(1, num_samples, 1)  # Expand for num_samples\n","        else:\n","            mu = torch.zeros((self.num_envs, num_samples, time_steps), device=self.device)\n","\n","        sigma = torch.ones_like(mu) * variance  # Add variance\n","        action_lists = torch.normal(mu, sigma)  # Sampled action sequences\n","\n","        action_lists = action_lists.view(-1, time_steps)  # Reshape to (num_envs * num_samples, time_steps)\n","        action_lists = torch.clamp(action_lists, -self.max_speed, self.max_speed)\n","\n","        # Track all trajectories\n","        trajectories = [batch_states.clone().unsqueeze(1)]  # Initial positions\n","        for t in range(time_steps):\n","            actions = action_lists[:, t]\n","            movement = self.f(batch_states, actions)\n","            batch_states = batch_states + movement * self.rate  # Move only by instantaneous action\n","            trajectories.append(batch_states.clone().unsqueeze(1))  # Store positions\n","            if centers is not None and radii is not None and verified_controls is not None and verified_indices is not None:\n","              # Only process environments that haven't switched yet\n","              need_switch = ~has_switched\n","\n","              # Compute mask and idx just for those\n","              idx_all = torch.full((self.num_envs * num_samples,), -1, dtype=torch.long, device=device)\n","              idx_all[need_switch] = find_hypercubes(batch_states[need_switch], centers, radii)\n","\n","              # Now determine who is eligible to switch\n","              eligible = need_switch & (idx_all > -1) & (verified_indices[idx_all] < time_steps - t)\n","              valid_indices = torch.nonzero(eligible, as_tuple=True)[0]\n","\n","              # Perform the switch\n","              if valid_indices.numel() > 0 and (time_steps - t - 1) > 0:\n","                  action_lists[valid_indices, t+1:] = verified_controls[idx_all[valid_indices], :time_steps - t - 1]\n","\n","              # Mark them as switched\n","              has_switched[valid_indices] = True\n","\n","\n","\n","        # Stack to get (num_envs * num_samples, time_steps+1, 2)\n","        trajectories = torch.cat(trajectories, dim=1)\n","\n","        trajectories = wrap_to_pi(trajectories)\n","        batch_states = wrap_to_pi(batch_states)\n","\n","        # Compute final distances for selection\n","        final_positions = batch_states\n","\n","        if self.eval_func:\n","          dist = torch.sqrt(((trajectories - self.target)**2 * torch.tensor([1.0, 0.01], device=device)).sum(dim=2))[:, 1:]\n","        else:\n","          dist = torch.linalg.norm(trajectories - self.target, dim=2, ord = float('inf'))[:, 1:]\n","\n","        r = torch.repeat_interleave(r, num_samples)\n","        firstpart = dist*torch.exp(self.alpha*t_eval)\n","        secondpart = torch.mul(r.unsqueeze(1), torch.exp(((self.L + self.alpha)*t_eval)).unsqueeze(0))\n","        alphadist = firstpart + secondpart\n","\n","        #final_distances = torch.linalg.norm(final_positions - self.target, dim=1, ord = float('inf'))\n","        alpha_distances = torch.min(alphadist, dim = 1).values\n","        taus = torch.argmin(alphadist, dim = 1)\n","\n","        # Reshape distances to (num_envs, num_samples)\n","        alpha_distances = alpha_distances.view(self.num_envs, num_samples)\n","        taus = taus.view(self.num_envs, num_samples)\n","\n","        # Find the best trajectory for each environment\n","        best_indices = torch.argmin(alpha_distances, dim=1)  # Best sample for each environment\n","        taus = taus[torch.arange(self.num_envs), best_indices]\n","\n","\n","        # Gather best actions and trajectories\n","        best_actions = action_lists.view(self.num_envs, num_samples, time_steps)[torch.arange(self.num_envs), best_indices]\n","        best_trajectories = trajectories.view(self.num_envs, num_samples, time_steps+1, self.dim)[torch.arange(self.num_envs), best_indices]\n","\n","        return best_actions, best_trajectories, alpha_distances.min(dim=1).values, taus\n","\n","\n","\n","\n","    def sample_trajectory_reuse_new(self, time_steps=5, control_seed = None, variance: int = 1, num_samples: int = 1, r = None, centers = None, radii = None, verified_alphas = None, verified_controls = None, verified_indices = None, splits = None, unverified_centers = None, unverified_radii = None):\n","        \"\"\" Generate batch trajectories for multiple sampled controls and select the best one.\n","\n","        - If `control_seed` is provided, it should have shape (num_envs, time_steps, 2).\n","        - Otherwise, random controls are sampled from a normal distribution.\n","\n","        Returns:\n","        - Best control sequence (num_envs, time_steps, 2) -> **Direct input for `trajectories`**\n","        - Best trajectory per environment (num_envs, time_steps+1, 2)\n","        - Minimum final distance per environment (num_envs,)\n","        \"\"\"\n","\n","        #variance = self.max_speed/3\n","        # Initialize batch states (only positions)\n","        t_eval = torch.linspace(0.0, time_steps*self.rate, steps=time_steps, device = device)\n","        if r == None:\n","          r = torch.zeros_like(t_eval)\n","        r = torch.repeat_interleave(r, num_samples)\n","        # Initialize batch states (only positions)\n","        batch_states = self.states.clone().repeat_interleave(num_samples, dim = 0)  # Shape: (num_envs * num_samples, 2)\n","        if splits is not None:\n","          has_switched = (splits < 5).repeat_interleave(num_samples, dim = 0)\n","\n","        # Each index i maps to class: i // n\n","        class_ids = torch.arange(self.num_envs * num_samples, device=device) // num_samples  # shape: (total,)\n","\n","        # Track which classes have already succeeded\n","        class_success = torch.zeros(self.num_envs, dtype=torch.bool, device=device)\n","\n","        # Track which entries are still active (not yet masked out)\n","        active_mask = torch.ones(self.num_envs * num_samples, dtype=torch.bool, device=device)\n","        taus = torch.zeros(self.num_envs, dtype=torch.long, device=device)\n","        best_indices = torch.zeros(self.num_envs, dtype=torch.long, device=device)\n","\n","\n","\n","        # Handle control seeding\n","        if control_seed is not None:\n","            assert control_seed.shape == (self.num_envs, time_steps), \\\n","                \"control_seed must have shape (num_envs, time_steps, 2)\"\n","            mu = control_seed.unsqueeze(1).repeat(1, num_samples, 1)  # Expand for num_samples\n","        else:\n","            mu = torch.zeros((self.num_envs, num_samples, time_steps), device=self.device)\n","\n","        sigma = torch.ones_like(mu) * variance  # Add variance\n","        action_lists = torch.normal(mu, sigma)  # Sampled action sequences\n","\n","        action_lists = action_lists.view(-1, time_steps)  # Reshape to (num_envs * num_samples, time_steps)\n","        action_lists = torch.clamp(action_lists, -self.max_speed, self.max_speed)\n","\n","        # Track all trajectories\n","        trajectories = [batch_states.clone().unsqueeze(1)]  # Initial positions\n","        for t in range(time_steps):\n","            active_indices = active_mask.nonzero(as_tuple=True)[0]\n","            actions = action_lists[active_indices, t]\n","            movement = self.f(batch_states[active_indices], actions)\n","            batch_states[active_indices] = batch_states[active_indices] + movement * self.rate  # Move only by instantaneous action\n","            trajectories.append(batch_states.clone().unsqueeze(1))  # Store positions\n","            if torch.min(r)*np.exp(L*t*self.rate) < 2*torch.max(radii) and centers is not None and radii is not None and verified_controls is not None and verified_indices is not None:\n","              # Only process environments that haven't switched yet\n","              torch.cuda.empty_cache()\n","              gc.collect()\n","              idx = check_parallel(batch_states[active_indices], t, r[active_indices], self.alpha,\n","                               self.L, self.rate, num_samples,\n","                               unverified_centers, unverified_radii,\n","                               centers, radii,\n","                               verified_alphas, verified_indices, torch.cat(trajectories, dim = 1))\n","\n","              global_success_indices = active_indices[idx]\n","              global_success_classes = class_ids[global_success_indices]\n","\n","              # Unique classes that succeeded this step and haven't succeeded before\n","              new_success_class_mask = ~class_success[global_success_classes]\n","              new_success_indices = global_success_indices[new_success_class_mask]\n","              new_success_classes = global_success_classes[new_success_class_mask]\n","\n","              # Update: mark class as succeeded\n","              class_success[new_success_classes] = True\n","\n","              # Save *one* successful index for each class (first seen here)\n","              best_indices[new_success_classes] = new_success_indices\n","\n","              taus[new_success_classes] = t\n","\n","              # Deactivate all samples from succeeded classes\n","              active_mask = active_mask & (~class_success[class_ids])\n","\n","\n","\n","        mask = ~class_success\n","        idxmask = (mask[class_ids]).nonzero(as_tuple=True)[0]\n","\n","\n","        # Stack to get (num_envs * num_samples, time_steps+1, 2)\n","        trajectories = torch.cat(trajectories, dim=1)\n","\n","        trajectories = wrap_to_pi(trajectories)\n","        batch_states = wrap_to_pi(batch_states)\n","\n","        # Compute final distances for selection\n","        final_positions = batch_states[idxmask]\n","\n","        if self.eval_func:\n","          dist = torch.sqrt(((trajectories - self.target)**2 * torch.tensor([1.0, 0.01], device=device)).sum(dim=2))[:, 1:]\n","        else:\n","          dist = torch.linalg.norm(trajectories - self.target, dim=2, ord = float('inf'))[:, 1:]\n","\n","        firstpart = dist*torch.exp(self.alpha*t_eval)\n","        secondpart = torch.mul(r.unsqueeze(1), torch.exp(((self.L + self.alpha)*t_eval)).unsqueeze(0))\n","        alphadist = firstpart + secondpart\n","\n","        #final_distances = torch.linalg.norm(final_positions - self.target, dim=1, ord = float('inf'))\n","        alpha_distances = torch.min(alphadist, dim = 1).values\n","        newtaus = torch.argmin(alphadist, dim = 1)\n","\n","        # Reshape distances to (num_envs, num_samples)\n","        alpha_distances = alpha_distances.view(self.num_envs, num_samples)\n","        newtaus = newtaus.view(self.num_envs, num_samples)\n","\n","        # Find the best trajectory for each environment\n","        best_indices[mask] = torch.argmin(alpha_distances, dim=1)[mask]  # Best sample for each environment\n","        best_indices = best_indices % num_samples\n","\n","        taus[mask] = newtaus[torch.arange(self.num_envs), best_indices][mask]\n","\n","        # Gather best actions and trajectories\n","        best_actions = action_lists.view(self.num_envs, num_samples, time_steps)[torch.arange(self.num_envs), best_indices]\n","        best_trajectories = trajectories.view(self.num_envs, num_samples, time_steps+1, self.dim)[torch.arange(self.num_envs), best_indices]\n","\n","        return best_actions, best_trajectories, alpha_distances.min(dim=1).values, taus, class_success\n","\n","    def trajectories(self, controls):\n","        \"\"\"\n","        Run forward only on environments where controls are not sentinel for 'None'.\n","\n","        Parameters:\n","        - controls (torch.Tensor): Shape (num_envs, time_steps)\n","\n","        Returns:\n","        - trajectories (torch.Tensor): Shape (num_envs, time_steps+1, 2)\n","        \"\"\"\n","        assert controls.shape[0] == self.num_envs, \"Controls must match (num_envs, time_steps)\"\n","        time_steps = controls.shape[1]\n","        batch_states = self.states.clone()  # shape (num_envs, 2)\n","        trajectories = [batch_states.clone().unsqueeze(1)]  # start with initial state\n","\n","        for t in range(time_steps):\n","            actions = controls[:, t]\n","            mask = actions != sentinel  # shape (num_envs,), True where control is valid\n","\n","            next_states = batch_states.clone()  # ← Clone before modification\n","            if mask.any():\n","                movement = self.f(batch_states[mask], actions[mask])  # only apply f to valid envs\n","                next_states[mask] += movement * self.rate\n","\n","            trajectories.append(next_states.unsqueeze(1))  # ← Append the consistent snapshot\n","            batch_states = next_states  # ← Move to next timestep\n","\n","        return torch.cat(trajectories, dim=1)  # Shape: (num_envs, time_steps+1, 2)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":117,"status":"ok","timestamp":1750702319734,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"-9-2gMIRgQ65"},"outputs":[],"source":["from typing_extensions import final\n","import torch\n","import numpy as np\n","import itertools\n","\n","class PendulumEnv_Old:\n","    def __init__(self,\n","                 num_envs=10,    # Number of parallel environments\n","                 target = None,   # Allow custom target location\n","                 max_radius=5.0,         # Define boundary size\n","                 max_speed=1.0,           # Limit agent's velocity\n","                 precision = 0.03,        # Define the allowed distance from the target\n","                 step_penalization = 2,   # How strongly each step taken is penalized\n","                 rate = 0.1,              # Rate of movement\n","                 field_function = inverted_pendulum_2d_torch,  # Allow different field functions\n","                 reward_type=\"distance\",   # Allow different reward strategies\n","                 dimension = 2,\n","                 max_starting_speed = 1.0,\n","                 min_alpha = 0.01,\n","                 L = 5,\n","                 eval_func = False\n","                ):\n","\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.num_envs = num_envs\n","\n","        self.max_speed = max_speed\n","        self.max_radius = max_radius\n","        self.precision = precision\n","        self.step_penalization = step_penalization\n","        self.rate = rate\n","        self.field_function = field_function\n","        self.reward_type = reward_type\n","        self.dim = dimension\n","        self.f = field_function\n","        self.max_starting_speed = max_starting_speed\n","        self.alpha = min_alpha\n","        self.L = L\n","        self.eval_func = eval_func\n","\n","        if target == None:\n","          self.target = torch.zeros(self.dim, device = self.device)\n","        else:\n","          self.target = torch.tensor(target, device=self.device, dtype=torch.float32)\n","\n","\n","        # Initialize states (x, y, vx, vy) for all environments in parallel\n","        self.states = self.sample_points_in_circle(self.num_envs).to(self.device)\n","        self.states = self.sample_points_in_hypercube(self.num_envs).to(self.device)\n","\n","    def sample_points_in_circle(self, batch_size):\n","        \"\"\" Sample random starting points within a circle of max_radius. \"\"\"\n","\n","        # Step 1: Sample from normal distribution (for uniform direction)\n","        points = torch.randn((batch_size, self.dim), device=self.device)\n","\n","        # Step 2: Normalize to lie on the unit n-sphere\n","        points /= torch.norm(points, dim=1, keepdim=True)\n","\n","        # Step 3: Sample radii with correct volume scaling\n","        radii = torch.rand((batch_size, 1), device=self.device) ** (1 / self.dim)  # Ensure uniform density\n","\n","        # Step 4: Scale points by radii\n","        return self.max_radius * points * radii\n","\n","    def sample_points(self, batch_size):\n","        \"\"\" Sample random starting points. \"\"\"\n","\n","        # Sample each coordinate independently from U(-1, 1)\n","        points = (torch.rand((batch_size, self.dim), device=self.device) * 2 - 1)  # Rescale [0,1] to [-1,1]\n","\n","        points = points * torch.tensor([self.max_radius, self.max_starting_speed], device=self.device)\n","\n","        return points\n","\n","    def sample_points_in_hypercube(self, batch_size):\n","        \"\"\" Sample random starting points uniformly within a hypercube. \"\"\"\n","\n","        # Sample each coordinate independently from U(-1, 1)\n","        points = (torch.rand((batch_size, self.dim), device=self.device) * 2 - 1)  # Rescale [0,1] to [-1,1]\n","\n","        # Scale by max_radius\n","        return self.max_radius * points\n","\n","\n","    def reset(self, seed = None):\n","        \"\"\" Reset all environments in parallel. \"\"\"\n","        if seed is not None:\n","          self.states = seed\n","        else:\n","          self.states = self.sample_points_in_hypercube(self.num_envs)\n","\n","\n","    def step(self, actions):\n","        \"\"\" Step function that updates all environments in parallel. \"\"\"\n","        actions = torch.clamp(actions, -self.max_speed, self.max_speed)\n","\n","        movement = self.f(self.states, actions)\n","        self.states += movement * self.rate  # Update positions\n","\n","        distances = torch.linalg.norm(self.states - self.target, dim=1, ord = float('inf'))\n","        rewards = -distances - self.step_penalization * self.rate\n","\n","        # Out-of-bounds penalty\n","        out_of_bounds = distances > self.max_radius\n","        rewards[out_of_bounds] = -10000\n","\n","        # Check if agents have reached the target\n","        done = distances < self.precision\n","        rewards[done] += 10000  # Bonus for reaching target\n","\n","        stop = torch.logical_or(done,out_of_bounds)\n","\n","        return self.states.clone(), rewards, stop\n","\n","    def sample_trajectory(self, time_steps=5, control_seed = None, variance: int = 1, num_samples: int = 1, r = None):\n","        \"\"\" Generate batch trajectories for multiple sampled controls and select the best one.\n","\n","        - If `control_seed` is provided, it should have shape (num_envs, time_steps, 2).\n","        - Otherwise, random controls are sampled from a normal distribution.\n","\n","        Returns:\n","        - Best control sequence (num_envs, time_steps, 2) -> **Direct input for `trajectories`**\n","        - Best trajectory per environment (num_envs, time_steps+1, 2)\n","        - Minimum final distance per environment (num_envs,)\n","        \"\"\"\n","\n","\n","        # Initialize batch states (only positions)\n","        t_eval = torch.linspace(0.0, time_steps*self.rate, steps=time_steps, device = device)\n","        if r == None:\n","          r = torch.zeros_like(t_eval)\n","        # Initialize batch states (only positions)\n","        batch_states = self.states.clone().repeat_interleave(num_samples, dim = 0)  # Shape: (num_envs * num_samples, 4)\n","\n","        # Handle control seeding\n","        if control_seed is not None:\n","            assert control_seed.shape == (self.num_envs, time_steps), \\\n","                \"control_seed must have shape (num_envs, time_steps, 2)\"\n","            mu = control_seed.unsqueeze(1).repeat(1, num_samples, 1)  # Expand for num_samples\n","        else:\n","            mu = torch.zeros((self.num_envs, num_samples, time_steps), device=self.device)\n","\n","        sigma = torch.ones_like(mu) * variance  # Add variance\n","        action_lists = torch.normal(mu, sigma)  # Sampled action sequences\n","\n","        action_lists = action_lists.view(-1, time_steps)  # Reshape to (num_envs * num_samples, time_steps)\n","        action_lists = torch.clamp(action_lists, -self.max_speed, self.max_speed)\n","\n","        # Track all trajectories\n","        trajectories = [batch_states.clone().unsqueeze(1)]  # Initial positions\n","        for t in range(time_steps):\n","            actions = action_lists[:, t]\n","            movement = self.f(batch_states, actions)\n","            batch_states = batch_states + movement * self.rate  # Move only by instantaneous action\n","            trajectories.append(batch_states.clone().unsqueeze(1))  # Store positions\n","\n","        # Stack to get (num_envs * num_samples, time_steps+1, 2)\n","        trajectories = torch.cat(trajectories, dim=1)\n","\n","        trajectories = wrap_to_pi(trajectories)\n","        batch_states = wrap_to_pi(batch_states)\n","\n","        # Compute final distances for selection\n","        final_positions = batch_states\n","\n","        if self.eval_func:\n","          dist = torch.sqrt(((trajectories - self.target)**2 * torch.tensor([1.0, 0.01], device=device)).sum(dim=2))[:, 1:]\n","        else:\n","          dist = torch.linalg.norm(trajectories - self.target, dim=2, ord = float('inf'))[:, 1:]\n","\n","        r = torch.repeat_interleave(r, num_samples)\n","        firstpart = dist*torch.exp(self.alpha*t_eval)\n","        secondpart = torch.mul(r.unsqueeze(1), torch.exp(((self.L + self.alpha)*t_eval)).unsqueeze(0))\n","        alphadist = firstpart + secondpart\n","\n","        #final_distances = torch.linalg.norm(final_positions - self.target, dim=1, ord = float('inf'))\n","        alpha_distances = torch.min(alphadist, dim = 1).values\n","        taus = torch.argmin(alphadist, dim = 1)\n","\n","        # Reshape distances to (num_envs, num_samples)\n","        alpha_distances = alpha_distances.view(self.num_envs, num_samples)\n","        taus = taus.view(self.num_envs, num_samples)\n","\n","        # Find the best trajectory for each environment\n","        best_indices = torch.argmin(alpha_distances, dim=1)  # Best sample for each environment\n","        taus = taus[torch.arange(self.num_envs), best_indices]\n","\n","\n","        # Gather best actions and trajectories\n","        best_actions = action_lists.view(self.num_envs, num_samples, time_steps)[torch.arange(self.num_envs), best_indices]\n","        best_trajectories = trajectories.view(self.num_envs, num_samples, time_steps+1, self.dim)[torch.arange(self.num_envs), best_indices]\n","\n","        return best_actions, best_trajectories, alpha_distances.min(dim=1).values, taus\n","\n","\n","\n","    def trajectories(self, controls):\n","        \"\"\"\n","        Run forward only on environments where controls are not sentinel for 'None'.\n","\n","        Parameters:\n","        - controls (torch.Tensor): Shape (num_envs, time_steps)\n","\n","        Returns:\n","        - trajectories (torch.Tensor): Shape (num_envs, time_steps+1, 2)\n","        \"\"\"\n","        assert controls.shape[0] == self.num_envs, \"Controls must match (num_envs, time_steps)\"\n","        time_steps = controls.shape[1]\n","        batch_states = self.states.clone()  # shape (num_envs, 2)\n","        trajectories = [batch_states.clone().unsqueeze(1)]  # start with initial state\n","\n","        for t in range(time_steps):\n","            actions = controls[:, t]\n","            mask = actions != sentinel  # shape (num_envs,), True where control is valid\n","\n","            next_states = batch_states.clone()  # ← Clone before modification\n","            if mask.any():\n","                movement = self.f(batch_states[mask], actions[mask])  # only apply f to valid envs\n","                next_states[mask] += movement * self.rate\n","\n","            trajectories.append(next_states.unsqueeze(1))  # ← Append the consistent snapshot\n","            batch_states = next_states  # ← Move to next timestep\n","\n","        return torch.cat(trajectories, dim=1)  # Shape: (num_envs, time_steps+1, 2)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1750702319740,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"tqlsi7sAPt-d"},"outputs":[],"source":["import torch\n","\n","def findpath_pendulum(env,\n","             seeds=None,  # Now handles multiple starting points (batch)\n","             countermax: int = 1,\n","             num_samples: int = 1000,\n","             variance: int = 1,\n","             time_steps: int = 20,\n","             control_seed=None,\n","             r = None):\n","    \"\"\"\n","    Parallelized version of findpath, optimizing multiple paths simultaneously.\n","\n","    Parameters:\n","    - env: ParallelPointNavigationEnv\n","    - seeds: Tensor of shape (num_envs, 2) specifying multiple start points\n","    - countermax: Max iterations without improvement before stopping\n","    - num_samples: Number of sampled controls per iteration\n","    - variance: Noise variance in control sampling\n","    - time_steps: Number of steps per trajectory\n","    - control_seed: Initial control guess (num_envs, time_steps, 2)\n","\n","    Returns:\n","    - Best control sequence (num_envs, time_steps, 2)\n","    - Number of iterations per environment\n","    \"\"\"\n","\n","    device = env.device\n","    num_envs = env.num_envs\n","\n","    # If no seed is provided, generate random ones\n","    if seeds is None:\n","        seeds = env.sample_points_in_hypercube(num_envs).to(device)  # Shape: (num_envs, 2)\n","\n","    env.states = seeds.clone()  # Reset all environments to their start positions\n","\n","    winner = control_seed if control_seed is not None else None\n","    endval = torch.full((num_envs,), float('inf'), device=device)  # Store best distances\n","    counter = torch.zeros(num_envs, device=device)  # Counter for convergence\n","    iters = torch.zeros(num_envs, dtype=torch.int32, device=device)  # Iteration count per env\n","    realtaus = torch.zeros(num_envs, dtype=torch.int32, device=device)\n","\n","    while torch.any(counter < countermax):  # Stop when all envs meet countermax\n","        env.states = seeds.clone()  # Reset start positions\n","        sol_actions, sol_trajectories, sol_distances, taus = env.sample_trajectory(\n","              time_steps=time_steps, control_seed=winner, num_samples=num_samples, variance=variance, r = r\n","        )\n","\n","\n","        # Update control sequences based on improvement\n","        improved = sol_distances < endval\n","        endval[improved] = sol_distances[improved]\n","        winner = torch.where(improved.unsqueeze(-1), sol_actions, winner if winner is not None else sol_actions)\n","        realtaus = torch.where(improved, taus, realtaus)\n","\n","\n","        # Update counter: If no improvement, increment; else, reset\n","        counter[improved] = 0\n","        counter[~improved] += 1\n","\n","        iters += 1  # Count total iterations\n","\n","    env.states = seeds.clone()  # Reset one final time\n","    return winner, iters, realtaus\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":41,"status":"ok","timestamp":1750702319782,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"43M_BgflVAX4"},"outputs":[],"source":["import torch\n","\n","def findpath_pendulum_reuse(env,\n","             seeds=None,  # Now handles multiple starting points (batch)\n","             countermax: int = 1,\n","             num_samples: int = 1000,\n","             variance: int = 1,\n","             time_steps: int = 20,\n","             control_seed=None,\n","             r = None,\n","             centers = None,\n","             radii = None,\n","             verified_alphas = None,\n","             verified_controls = None,\n","             verified_indices = None,\n","             splits = None,\n","             unverified_centers = None,\n","             unverified_radii = None):\n","    \"\"\"\n","    Parallelized version of findpath, optimizing multiple paths simultaneously.\n","\n","    Parameters:\n","    - env: ParallelPointNavigationEnv\n","    - seeds: Tensor of shape (num_envs, 2) specifying multiple start points\n","    - countermax: Max iterations without improvement before stopping\n","    - num_samples: Number of sampled controls per iteration\n","    - variance: Noise variance in control sampling\n","    - time_steps: Number of steps per trajectory\n","    - control_seed: Initial control guess (num_envs, time_steps, 2)\n","\n","    Returns:\n","    - Best control sequence (num_envs, time_steps, 2)\n","    - Number of iterations per environment\n","    \"\"\"\n","\n","    device = env.device\n","    num_envs = env.num_envs\n","\n","    # If no seed is provided, generate random ones\n","    if seeds is None:\n","        seeds = env.sample_points_in_hypercube(num_envs).to(device)  # Shape: (num_envs, 2)\n","\n","    env.states = seeds.clone()  # Reset all environments to their start positions\n","\n","    winner = control_seed if control_seed is not None else None\n","    endval = torch.full((num_envs,), float('inf'), device=device)  # Store best distances\n","    counter = torch.zeros(num_envs, device=device)  # Counter for convergence\n","    iters = torch.zeros(num_envs, dtype=torch.int32, device=device)  # Iteration count per env\n","    realtaus = torch.zeros(num_envs, dtype=torch.int32, device=device)\n","\n","    while torch.any(counter < countermax):  # Stop when all envs meet countermax\n","        env.states = seeds.clone()  # Reset start positions\n","        if centers is not None and radii is not None and verified_controls is not None and verified_indices is not None and splits is not None:\n","          sol_actions, sol_trajectories, sol_distances, taus, reusing = env.sample_trajectory_reuse_new(\n","            time_steps=time_steps, control_seed=winner, num_samples=num_samples, variance=variance, r = r, centers = centers, radii = radii, verified_alphas = verified_alphas, verified_controls = verified_controls, verified_indices = verified_indices, splits = splits, unverified_centers = unverified_centers, unverified_radii = unverified_radii\n","          )\n","        else:\n","          sol_actions, sol_trajectories, sol_distances, taus = env.sample_trajectory(\n","              time_steps=time_steps, control_seed=winner, num_samples=num_samples, variance=variance, r = r\n","          )\n","\n","\n","        # Update control sequences based on improvement\n","        improved = sol_distances < endval\n","        endval[improved] = sol_distances[improved]\n","        winner = torch.where(improved.unsqueeze(-1), sol_actions, winner if winner is not None else sol_actions)\n","        realtaus = torch.where(improved, taus, realtaus)\n","\n","\n","        # Update counter: If no improvement, increment; else, reset\n","        counter[improved] = 0\n","        counter[~improved] += 1\n","\n","        iters += 1  # Count total iterations\n","\n","    env.states = seeds.clone()  # Reset one final time\n","    return winner, iters, realtaus, reusing\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1750702319786,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"EWZnZKiBFwna"},"outputs":[],"source":["import torch\n","\n","def findpath_pendulum_RL(env,\n","             seeds=None,  # Now handles multiple starting points (batch)\n","             countermax: int = 1,\n","             num_samples: int = 1000,\n","             variance: int = 1,\n","             time_steps: int = 20,\n","             control_seed=None,\n","             r = None,\n","             model = None):\n","    \"\"\"\n","    Parallelized version of findpath, optimizing multiple paths simultaneously.\n","\n","    Parameters:\n","    - env: ParallelPointNavigationEnv\n","    - seeds: Tensor of shape (num_envs, 2) specifying multiple start points\n","    - countermax: Max iterations without improvement before stopping\n","    - num_samples: Number of sampled controls per iteration\n","    - variance: Noise variance in control sampling\n","    - time_steps: Number of steps per trajectory\n","    - control_seed: Initial control guess (num_envs, time_steps, 2)\n","\n","    Returns:\n","    - Best control sequence (num_envs, time_steps, 2)\n","    - Number of iterations per environment\n","    \"\"\"\n","\n","    if model == None:\n","      raise Exception(\"No model provided\")\n","    # If no seed is provided, generate random ones\n","    if seeds is None:\n","        seeds = env.sample_points_in_hypercube(num_envs).to(device)  # Shape: (num_envs, 2)\n","\n","    winners = []\n","    trajectories = []\n","    for seed in seeds:\n","      env.reset()\n","      winner = []\n","      env.unwrapped.state = seed.cpu().numpy()\n","      obs = env.unwrapped._get_obs()\n","      traj = [env.unwrapped.state]\n","      for i in range(time_steps):\n","        action, _ = model.predict(obs)\n","        winner.append(action)\n","        obs, reward, _, _, _ = env.step(action)\n","        traj.append(env.unwrapped.state)\n","      winners.append(winner)\n","      trajectories.append(traj)\n","\n","    winners = torch.tensor(winners, device = device).squeeze(-1)\n","    trajectories = torch.tensor(trajectories, device = device)\n","    return winners, trajectories\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":52,"status":"ok","timestamp":1750702319813,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"Xlc5uysCPxB4"},"outputs":[],"source":["import gc\n","import math\n","import itertools\n","import time\n","\n","def pendulum_algo(d: int = 2, R: float = 2, epsilon: float = 0.01, L: float = 1.8, tau: float = 2, min_alpha: float = 0, batch_size: int = 500, dt: float = 0.1, speed: float = 2, max_splits: int = 3, num_samples: int = 1000, function = None, target = None, reuse = False, wrapper: bool = True) -> list:\n","\n","  if target == None:\n","    target = torch.zeros(d)\n","  start = time.time()\n","\n","  # Initialize the starting grid\n","\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","  prod_fun = torch.vmap(torch.cartesian_prod)\n","\n","  num_layers = math.ceil(math.log(R/epsilon, 3))\n","\n","  points = torch.tensor([round(2*epsilon*3**i, 4) for i in range(num_layers)], device = device)\n","  points = torch.vstack([-points, torch.zeros_like(points), points]).transpose(0,1).reshape(-1, 3)\n","  if d == 2:\n","    points = prod_fun(points, points).reshape(-1, d)\n","  elif d == 3:\n","    points = prod_fun(points, points, points).reshape(-1, d)\n","  elif d == 4:\n","    points = prod_fun(points, points, points, points).reshape(-1, d)\n","  elif d == 5:\n","    points = prod_fun(points, points, points, points, points).reshape(-1, d)\n","  elif d == 6:\n","    points = prod_fun(points, points, points, points, points, points).reshape(-1, d)\n","  points = points[torch.where((points != 0).any(dim = 1))].to(device)\n","\n","  radius = [[round(epsilon*3**i, 4)]* (3**d - 1) for i in range (num_layers)]\n","  radius = list(itertools.chain.from_iterable(radius))\n","  radius = torch.tensor(radius, device = device)\n","  splits = torch.zeros_like(radius)\n","  controls = torch.zeros(len(points), int(tau/dt)).to(device)\n","  #radius = torch.max(torch.min(radius, (torch.abs(points[:,2])-epsilon/2)), torch.zeros_like(radius))\n","\n","  # for i in range(4,num_layers):\n","  #   splits[i*(3**d-1):]  =  splits[i*(3**d-1):] - 1\n","\n","\n","  verified_points = torch.tensor([np.zeros(d)]).to(device)\n","  verified_radius = torch.tensor([epsilon]).to(device)\n","  verified_controls = controls[0].unsqueeze(0).to(device)\n","  verified_alphas = torch.tensor([0]).to(device)\n","  verified_indices = torch.tensor([1]).to(device)\n","\n","  if wrapper:\n","    loc_mask = torch.abs(points[:, 0]) - radius <= torch.pi\n","    points = points[loc_mask]\n","    radius = radius[loc_mask]\n","    controls = controls[loc_mask]\n","    splits = splits[loc_mask]\n","\n","\n","\n","  t_eval = torch.linspace(0.0, tau, steps=int(tau/dt + 1), device = device)\n","\n","\n","  while len(points > 0):\n","    print('Points:' + str(len(points)))\n","    split_points = points[:batch_size]\n","    split_radius = radius[:batch_size]\n","    split_controls = controls[:batch_size]\n","    split_splits = splits[:batch_size]\n","    env = PendulumEnv(num_envs = len(split_points), dimension = d, rate = dt, max_speed = speed, min_alpha = min_alpha, L = L, eval_func = False, field_function = function)\n","    points = points[batch_size:]\n","    radius = radius[batch_size:]\n","    controls = controls[batch_size:]\n","    splits = splits[batch_size:]\n","    env.reset(split_points)\n","    if reuse:\n","      winner, _, taus, reusing = findpath_pendulum(env = env, seeds = split_points, time_steps = int(tau/dt), control_seed = split_controls, num_samples = num_samples, r = split_radius, centers = verified_points, radii = verified_radius, verified_controls=verified_controls, verified_indices=verified_indices, splits = split_splits, unverified_centers = torch.cat(points, split_points), unverified_radii = torch.cat(radius, split_radius))\n","    else:\n","      winner, _, taus = findpath_pendulum(env = env, seeds = split_points, time_steps = int(tau/dt), control_seed = split_controls, num_samples = num_samples, r = split_radius)\n","    env.reset(split_points)\n","    sol = env.trajectories(winner)\n","    sol = torch.linalg.norm(wrap_to_pi(sol), dim = 2, ord = float('inf'))\n","\n","    #sol = torch.linalg.norm(sol, dim = 2, ord = float('inf'))\n","    alpha, indices = search_alpha_parallel(sol, torch.linalg.norm(split_points, dim = 1, ord = float('inf')), split_radius, t_eval, L)\n","    #alpha, indices = search_alpha_parallel(sol, torch.linalg.norm(split_points, dim = 1, ord = float('inf')), split_radius, t_eval, L)\n","    #condition, indices = certify_alpha(sol, torch.linalg.norm(split_points, dim = 1, ord = float('inf')), split_radius, t_eval, L, min_alpha)\n","    indices = indices + 1\n","    #mask = condition\n","    mask = alpha > min_alpha\n","    verified_points = torch.cat((verified_points, split_points[mask]), dim = 0)\n","    verified_radius = torch.cat((verified_radius, split_radius[mask]), dim = 0)\n","    verified_controls = torch.cat((verified_controls, winner[mask]), dim = 0)\n","    verified_alphas = torch.cat((verified_alphas, alpha[mask]), dim = 0)\n","    verified_indices = torch.cat((verified_indices, indices[mask]), dim = 0)\n","    if len(split_radius[~mask]) > 0:\n","        temp_combinations = torch.vstack([-2/3*split_radius[~mask], torch.zeros_like(split_radius[~mask]), 2/3*split_radius[~mask]]).transpose(0,1).reshape(-1, 3)\n","        if d == 2:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 3:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 4:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 5:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 6:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations, temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        new_points = split_points[~mask].repeat_interleave(3**d, dim = 0) + temp_combinations\n","        new_radius = split_radius[~mask].repeat_interleave(3**d, dim = 0)*1/3\n","        new_splits = split_splits[~mask].repeat_interleave(3**d, dim = 0) + 1\n","        new_controls = winner[~mask].repeat_interleave(3**d, dim=0)\n","\n","        points = torch.cat((points, new_points), 0)\n","        radius = torch.cat((radius, new_radius), 0)\n","        controls = torch.cat((controls, new_controls), 0)\n","        splits = torch.cat((splits, new_splits), 0)\n","        mask1 = torch.linalg.norm(points, dim = 1, ord = float('inf')) - radius <= R\n","        mask2 = torch.abs(points[:, 0]) - radius <= torch.pi\n","        mask3 = splits < max_splits\n","        mask = mask1 & mask2 & mask3\n","        points = points[mask]\n","        radius = radius[mask]\n","        controls = controls[mask]\n","        splits = splits[mask]\n","\n","\n","  print(time.time() - start)\n","\n","  return verified_points, verified_radius, verified_controls, verified_alphas, verified_indices\n","  #return verified_points, verified_radius, verified_controls, verified_indices\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":51,"status":"ok","timestamp":1750702319864,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"9LLj8cpiVOnD"},"outputs":[],"source":["import gc\n","import math\n","import itertools\n","import time\n","\n","def pendulum_algo_reuse(d: int = 2, R: float = 2, epsilon: float = 0.01, L: float = 1.8, tau: float = 2, min_alpha: float = 0, batch_size: int = 500, dt: float = 0.1, speed: float = 2, max_splits: int = 3, num_samples: int = 1000, function = None, target = None, reuse = False, wrapper: bool = True, savetime: int = 1800, cont: bool = False, conttime: int = None, contlist = None) -> list:\n","\n","  if target == None:\n","    target = torch.zeros(d)\n","  start = time.time()\n","  startreset = time.time()\n","  resetcount = 1\n","\n","  # Initialize the starting grid\n","\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","  prod_fun = torch.vmap(torch.cartesian_prod)\n","\n","  num_layers = math.ceil(math.log(R/epsilon, 3))\n","\n","  points = torch.tensor([round(2*epsilon*3**i, 4) for i in range(num_layers)], device = device)\n","  points = torch.vstack([-points, torch.zeros_like(points), points]).transpose(0,1).reshape(-1, 3)\n","  if d == 2:\n","    points = prod_fun(points, points).reshape(-1, d)\n","  elif d == 3:\n","    points = prod_fun(points, points, points).reshape(-1, d)\n","  elif d == 4:\n","    points = prod_fun(points, points, points, points).reshape(-1, d)\n","  elif d == 5:\n","    points = prod_fun(points, points, points, points, points).reshape(-1, d)\n","  elif d == 6:\n","    points = prod_fun(points, points, points, points, points, points).reshape(-1, d)\n","  points = points[torch.where((points != 0).any(dim = 1))].to(device)\n","\n","  radius = [[round(epsilon*3**i, 4)]* (3**d - 1) for i in range (num_layers)]\n","  radius = list(itertools.chain.from_iterable(radius))\n","  radius = torch.tensor(radius, device = device)\n","  splits = torch.zeros_like(radius)\n","  controls = torch.zeros(len(points), int(tau/dt)).to(device)\n","  #radius = torch.max(torch.min(radius, (torch.abs(points[:,2])-epsilon/2)), torch.zeros_like(radius))\n","\n","  # for i in range(4,num_layers):\n","  #   splits[i*(3**d-1):]  =  splits[i*(3**d-1):] - 1\n","\n","\n","  verified_points = torch.tensor([np.zeros(d)]).to(device)\n","  verified_radius = torch.tensor([epsilon]).to(device)\n","  verified_controls = controls[0].unsqueeze(0).to(device)\n","  verified_alphas = torch.tensor([0]).to(device)\n","  verified_indices = torch.tensor([1]).to(device)\n","  unverified_points = None\n","  unverified_radius = None\n","\n","  if wrapper:\n","    loc_mask = torch.abs(points[:, 0]) - radius <= torch.pi\n","    points = points[loc_mask]\n","    radius = radius[loc_mask]\n","    controls = controls[loc_mask]\n","    splits = splits[loc_mask]\n","\n","  if cont:\n","    if conttime is not None:\n","      name = 'timesave_' + str(int(conttime))\n","      with open(name + \".pkl\", \"rb\") as f:\n","        verified_points, verified_radius, verified_controls, verified_alphas, verified_indices, points, radius, controls, splits, unverified_points, unverified_radius = pickle.load(f)\n","        resetcount = (conttime/savetime) + 1\n","    elif contlist is not None:\n","      verified_points, verified_radius, verified_controls, verified_alphas, verified_indices, points, radius = contlist\n","      controls = torch.zeros(len(points), int(tau/dt)).to(device)\n","      splits = torch.zeros_like(radius)\n","\n","\n","  t_eval = torch.linspace(0.0, tau, steps=int(tau/dt + 1), device = device)\n","\n","\n","  while len(points > 0):\n","    if time.time() - startreset > savetime:\n","      name = 'timesave_' + str(resetcount * savetime)\n","      with open(name + \".pkl\", \"wb\") as f:\n","        pickle.dump([verified_points, verified_radius, verified_controls, verified_alphas, verified_indices, points, radius, controls, splits, unverified_points, unverified_radius], f)\n","      files.download(name + \".pkl\")\n","      startreset = time.time()\n","      resetcount += 1\n","    print('Points:' + str(len(points)))\n","    split_points = points[:batch_size]\n","    split_radius = radius[:batch_size]\n","    split_controls = controls[:batch_size]\n","    split_splits = splits[:batch_size]\n","    env = PendulumEnv(num_envs = len(split_points), dimension = d, rate = dt, max_speed = speed, min_alpha = min_alpha, L = L, eval_func = False, field_function = function)\n","    points = points[batch_size:]\n","    radius = radius[batch_size:]\n","    controls = controls[batch_size:]\n","    splits = splits[batch_size:]\n","    env.reset(split_points)\n","    if reuse:\n","      winner, _, taus, reusing = findpath_pendulum_reuse(env = env, seeds = split_points, time_steps = int(tau/dt), control_seed = split_controls, num_samples = num_samples, r = split_radius, centers = verified_points, radii = verified_radius, verified_alphas = verified_alphas, verified_controls=verified_controls, verified_indices=verified_indices, splits = split_splits, unverified_centers = torch.cat((points, split_points)), unverified_radii = torch.cat((radius, split_radius)))\n","    else:\n","      winner, _, taus = findpath_pendulum(env = env, seeds = split_points, time_steps = int(tau/dt), control_seed = split_controls, num_samples = num_samples, r = split_radius)\n","    env.reset(split_points)\n","    sol = env.trajectories(winner)\n","    sol = torch.linalg.norm(wrap_to_pi(sol), dim = 2, ord = float('inf'))\n","    alpha = min_alpha * torch.ones_like(split_radius, device = device)\n","    indices = taus\n","    #sol = torch.linalg.norm(sol, dim = 2, ord = float('inf'))\n","    print(\"Reusing out of Batch: \", reusing.sum().item(), '/', len(split_points))\n","    if (~reusing).sum() > 0:\n","      alpha_calc, indices_calc = search_alpha_parallel(sol[~reusing], torch.linalg.norm(split_points, dim = 1, ord = float('inf'))[~reusing], split_radius[~reusing], t_eval, L)\n","      alpha[~reusing] = alpha_calc\n","      indices[~reusing] = indices_calc\n","    #alpha, indices = search_alpha_parallel(sol, torch.linalg.norm(split_points, dim = 1, ord = float('inf')), split_radius, t_eval, L)\n","    #condition, indices = certify_alpha(sol, torch.linalg.norm(split_points, dim = 1, ord = float('inf')), split_radius, t_eval, L, min_alpha)\n","    indices = indices + 1\n","    #mask = condition\n","    mask = alpha >= min_alpha\n","    verified_points = torch.cat((verified_points, split_points[mask]), dim = 0)\n","    verified_radius = torch.cat((verified_radius, split_radius[mask]), dim = 0)\n","    verified_controls = torch.cat((verified_controls, winner[mask]), dim = 0)\n","    verified_alphas = torch.cat((verified_alphas, alpha[mask]), dim = 0)\n","    verified_indices = torch.cat((verified_indices, indices[mask]), dim = 0)\n","    if len(split_radius[~mask]) > 0:\n","        temp_combinations = torch.vstack([-2/3*split_radius[~mask], torch.zeros_like(split_radius[~mask]), 2/3*split_radius[~mask]]).transpose(0,1).reshape(-1, 3)\n","        if d == 2:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 3:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 4:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 5:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 6:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations, temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        new_points = split_points[~mask].repeat_interleave(3**d, dim = 0) + temp_combinations\n","        new_radius = split_radius[~mask].repeat_interleave(3**d, dim = 0)*1/3\n","        new_splits = split_splits[~mask].repeat_interleave(3**d, dim = 0) + 1\n","        new_controls = winner[~mask].repeat_interleave(3**d, dim=0)\n","\n","        points = torch.cat((points, new_points), 0)\n","        radius = torch.cat((radius, new_radius), 0)\n","        controls = torch.cat((controls, new_controls), 0)\n","        splits = torch.cat((splits, new_splits), 0)\n","        mask1 = torch.linalg.norm(points, dim = 1, ord = float('inf')) - radius <= R\n","        mask2 = torch.abs(points[:, 0]) - radius <= torch.pi\n","        mask3 = splits < max_splits\n","        mask = mask1 & mask2 & mask3\n","        if torch.any(mask3 == False):\n","          if unverified_points == None:\n","            unverified_points = points[~mask]\n","            unverified_radius = radius[~mask]\n","          else:\n","            unverified_points = torch.cat((unverified_points, points[~mask]), dim = 0)\n","            unverified_radius = torch.cat((unverified_radius, radius[~mask]), dim = 0)\n","        points = points[mask]\n","        radius = radius[mask]\n","        controls = controls[mask]\n","        splits = splits[mask]\n","\n","\n","\n","  print(time.time() - start)\n","\n","  return verified_points, verified_radius, verified_controls, verified_alphas, verified_indices, unverified_points, unverified_radius\n","  #return verified_points, verified_radius, verified_controls, verified_indices\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1750702319871,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"ug3C791808Fh"},"outputs":[],"source":["import gc\n","import math\n","import itertools\n","import time\n","\n","def pendulum_algo_alt(d: int = 2, R: float = 2, epsilon: float = 0.01, L: float = 1.8, tau: float = 2, min_alpha: float = 0, batch_size: int = 500, dt: float = 0.1, speed: float = 2, max_splits: int = 3, num_samples: int = 1000, function = None, target = None, reuse = False) -> list:\n","\n","  if target == None:\n","    target = torch.zeros(d)\n","  start = time.time()\n","\n","  # Initialize the starting grid\n","\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","  prod_fun = torch.vmap(torch.cartesian_prod)\n","\n","  num_layers = math.ceil(math.log(R/epsilon, 3))\n","\n","  points = torch.tensor([round(2*epsilon*3**i, 4) for i in range(num_layers)], device = device)\n","  points = torch.vstack([-points, torch.zeros_like(points), points]).transpose(0,1).reshape(-1, 3)\n","  if d == 2:\n","    points = prod_fun(points, points).reshape(-1, d)\n","  elif d == 3:\n","    points = prod_fun(points, points, points).reshape(-1, d)\n","  elif d == 4:\n","    points = prod_fun(points, points, points, points).reshape(-1, d)\n","  elif d == 5:\n","    points = prod_fun(points, points, points, points, points).reshape(-1, d)\n","  elif d == 6:\n","    points = prod_fun(points, points, points, points, points, points).reshape(-1, d)\n","  points = points[torch.where((points != 0).any(dim = 1))].to(device)\n","\n","  radius = [[round(epsilon*3**i, 4)]* (3**d - 1) for i in range (num_layers)]\n","  radius = list(itertools.chain.from_iterable(radius))\n","  radius = torch.tensor(radius, device = device)\n","  splits = torch.zeros_like(radius)\n","  controls = torch.zeros(len(points), int(tau/dt)).to(device)\n","  #radius = torch.max(torch.min(radius, (torch.abs(points[:,2])-epsilon/2)), torch.zeros_like(radius))\n","\n","  # for i in range(5,num_layers):\n","  #   splits[i*(3**d-1):]  =  splits[i*(3**d-1):] - 1\n","\n","\n","  verified_points = torch.tensor([np.zeros(d)]).to(device)\n","  verified_radius = torch.tensor([epsilon]).to(device)\n","  verified_controls = controls[0].unsqueeze(0).to(device)\n","  verified_alphas = torch.tensor([0]).to(device)\n","  verified_indices = torch.tensor([1]).to(device)\n","\n","  t_eval = torch.linspace(0.0, tau, steps=int(tau/dt + 1), device = device)\n","\n","\n","  while len(points > 0):\n","    print('Points:' + str(len(points)))\n","    #points[points[:,2] == 0] += torch.tensor([0,0,0.0001,0]).to(device)\n","    split_points = points[:batch_size]\n","    split_radius = radius[:batch_size]\n","    split_controls = controls[:batch_size]\n","    split_splits = splits[:batch_size]\n","    env = PendulumEnv(num_envs = len(split_points), dimension = d, rate = dt, max_speed = speed, min_alpha = min_alpha, L = L, eval_func = False, field_function = function)\n","    points = points[batch_size:]\n","    radius = radius[batch_size:]\n","    controls = controls[batch_size:]\n","    splits = splits[batch_size:]\n","    env.reset(split_points)\n","    if reuse:\n","      winner, _, taus = findpath_pendulum(env = env, seeds = split_points, time_steps = int(tau/dt), control_seed = split_controls, num_samples = num_samples, r = split_radius, centers = verified_points, radii = verified_radius, verified_controls=verified_controls, verified_indices=verified_indices)\n","    else:\n","      winner, _, taus = findpath_pendulum(env = env, seeds = split_points, time_steps = int(tau/dt), control_seed = split_controls, num_samples = num_samples, r = split_radius)\n","    env.reset(split_points)\n","    sol = env.trajectories(winner)\n","    sol = torch.sqrt((wrap_to_pi(sol)**2 * torch.tensor([1.0, 0.01], device=device)).sum(dim=2))\n","    #alpha, indices = search_alpha_parallel(sol, sol[:, 0], split_radius, t_eval, L)\n","    condition, indices = certify_alpha(sol, sol[:, 0], split_radius, t_eval, L, min_alpha)\n","    indices = indices + 1\n","    mask = condition\n","    #mask = alpha > min_alpha\n","    verified_points = torch.cat((verified_points, split_points[mask]), dim = 0)\n","    verified_radius = torch.cat((verified_radius, split_radius[mask]), dim = 0)\n","    verified_controls = torch.cat((verified_controls, winner[mask]), dim = 0)\n","    #verified_alphas = torch.cat((verified_alphas, alpha[mask]), dim = 0)\n","    verified_indices = torch.cat((verified_indices, indices[mask]), dim = 0)\n","    if len(split_radius[~mask]) > 0:\n","        temp_combinations = torch.vstack([-2/3*split_radius[~mask], torch.zeros_like(split_radius[~mask]), 2/3*split_radius[~mask]]).transpose(0,1).reshape(-1, 3)\n","        if d == 2:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 3:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 4:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 5:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 6:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations, temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        new_points = split_points[~mask].repeat_interleave(3**d, dim = 0) + temp_combinations\n","        new_radius = split_radius[~mask].repeat_interleave(3**d, dim = 0)*1/3\n","        new_splits = split_splits[~mask].repeat_interleave(3**d, dim = 0) + 1\n","        new_controls = winner[~mask].repeat_interleave(3**d, dim=0)\n","\n","        points = torch.cat((points, new_points), 0)\n","        radius = torch.cat((radius, new_radius), 0)\n","        controls = torch.cat((controls, new_controls), 0)\n","        splits = torch.cat((splits, new_splits), 0)\n","        mask1 = torch.linalg.norm(points, dim = 1, ord = float('inf')) - radius <= R\n","        mask2 = torch.abs(points[:, 0]) - radius <= torch.pi\n","        mask3 = splits < max_splits\n","        mask = mask1 & mask2 & mask3\n","        points = points[mask]\n","        radius = radius[mask]\n","        controls = controls[mask]\n","        splits = splits[mask]\n","\n","\n","\n","  print(time.time() - start)\n","\n","  #return verified_points, verified_radius, verified_controls, verified_alphas, verified_indices\n","  return verified_points, verified_radius, verified_controls, verified_indices\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":98,"status":"ok","timestamp":1750702319970,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"ouVUd9NHFYAr"},"outputs":[],"source":["import gc\n","import math\n","import itertools\n","import time\n","\n","\n","def pendulum_algo_RL(d: int = 2, R: float = 2, epsilon: float = 0.01, L: float = 1.8, tau: float = 2, min_alpha: float = 0, batch_size: int = 500, dt: float = 0.1, speed: float = 2, max_splits: int = 3, num_samples: int = 1000, function = None, target = None, model = None, wrapper: bool = True) -> list:\n","\n","  if model == None:\n","    raise Exception('No model provided')\n","  if target == None:\n","    target = torch.zeros(d)\n","  start = time.time()\n","\n","  # Initialize the starting grid\n","\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","  prod_fun = torch.vmap(torch.cartesian_prod)\n","\n","  num_layers = math.ceil(math.log(R/epsilon, 3))\n","\n","  points = torch.tensor([round(2*epsilon*3**i, 4) for i in range(num_layers)], device = device)\n","  points = torch.vstack([-points, torch.zeros_like(points), points]).transpose(0,1).reshape(-1, 3)\n","  if d == 2:\n","    points = prod_fun(points, points).reshape(-1, d)\n","  elif d == 3:\n","    points = prod_fun(points, points, points).reshape(-1, d)\n","  elif d == 4:\n","    points = prod_fun(points, points, points, points).reshape(-1, d)\n","  elif d == 5:\n","    points = prod_fun(points, points, points, points, points).reshape(-1, d)\n","  elif d == 6:\n","    points = prod_fun(points, points, points, points, points, points).reshape(-1, d)\n","  points = points[torch.where((points != 0).any(dim = 1))].to(device)\n","\n","  radius = [[round(epsilon*3**i, 4)]* (3**d - 1) for i in range (num_layers)]\n","  radius = list(itertools.chain.from_iterable(radius))\n","  radius = torch.tensor(radius, device = device)\n","  splits = torch.zeros_like(radius)\n","  controls = torch.zeros(len(points), int(tau/dt)).to(device)\n","  #radius = torch.max(torch.min(radius, (torch.abs(points[:,2])-epsilon/2)), torch.zeros_like(radius))\n","\n","  for i in range(5,num_layers):\n","    splits[i*(3**d-1):]  =  splits[i*(3**d-1):] - 1\n","\n","  if wrapper:\n","    loc_mask = torch.abs(points[:, 0]) - radius <= torch.pi\n","    points = points[loc_mask]\n","    radius = radius[loc_mask]\n","    controls = controls[loc_mask]\n","    splits = splits[loc_mask]\n","\n","\n","  verified_points = torch.tensor([np.zeros(d)]).to(device)\n","  verified_radius = torch.tensor([epsilon]).to(device)\n","  verified_controls = controls[0].unsqueeze(0).to(device)\n","  verified_alphas = torch.tensor([0]).to(device)\n","  verified_indices = torch.tensor([1]).to(device)\n","\n","  t_eval = torch.linspace(0.0, tau, steps=int(tau/dt + 1), device = device)\n","\n","\n","  while len(points > 0):\n","    print('Points:' + str(len(points)))\n","    #points[points[:,2] == 0] += torch.tensor([0,0,0.0001,0]).to(device)\n","    split_points = points[:batch_size]\n","    split_radius = radius[:batch_size]\n","    split_controls = controls[:batch_size]\n","    split_splits = splits[:batch_size]\n","    env = gym.make(\"Pendulum-v1\")\n","    points = points[batch_size:]\n","    radius = radius[batch_size:]\n","    controls = controls[batch_size:]\n","    splits = splits[batch_size:]\n","    env.reset()\n","    winner, sol = findpath_pendulum_RL(env = env, seeds = split_points, time_steps = int(tau/dt), control_seed = split_controls, num_samples = num_samples, r = split_radius, model = model, field_function = function)\n","    #sol = torch.sqrt((wrap_to_pi(sol)**2 * torch.tensor([1.0, 0.2], device=device)).sum(dim=2))\n","    sol = torch.linalg.norm(sol, dim = 2, ord = float('inf'))\n","    alpha, indices = search_alpha_parallel(sol, torch.linalg.norm(split_points, dim = 1, ord = float('inf')), split_radius, t_eval, L)\n","    #alpha, indices = search_alpha_parallel(sol, torch.linalg.norm(split_points, dim = 1, ord = float('inf')), split_radius, t_eval, L)\n","    #condition, indices = certify_alpha(sol, torch.linalg.norm(split_points, dim = 1, ord = float('inf')), split_radius, t_eval, L, min_alpha)\n","    indices = indices + 1\n","    #mask = condition\n","    mask = alpha > min_alpha\n","    verified_points = torch.cat((verified_points, split_points[mask]), dim = 0)\n","    verified_radius = torch.cat((verified_radius, split_radius[mask]), dim = 0)\n","    verified_controls = torch.cat((verified_controls, winner[mask]), dim = 0)\n","    verified_alphas = torch.cat((verified_alphas, alpha[mask]), dim = 0)\n","    verified_indices = torch.cat((verified_indices, indices[mask]), dim = 0)\n","    if len(split_radius[~mask]) > 0:\n","        temp_combinations = torch.vstack([-2/3*split_radius[~mask], torch.zeros_like(split_radius[~mask]), 2/3*split_radius[~mask]]).transpose(0,1).reshape(-1, 3)\n","        if d == 2:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 3:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 4:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 5:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        elif d == 6:\n","          temp_combinations = prod_fun(temp_combinations, temp_combinations, temp_combinations, temp_combinations, temp_combinations, temp_combinations).reshape(-1, d)\n","        new_points = split_points[~mask].repeat_interleave(3**d, dim = 0) + temp_combinations\n","        new_radius = split_radius[~mask].repeat_interleave(3**d, dim = 0)*1/3\n","        new_splits = split_splits[~mask].repeat_interleave(3**d, dim = 0) + 1\n","        new_controls = winner[~mask].repeat_interleave(3**d, dim=0)\n","\n","        points = torch.cat((points, new_points), 0)\n","        radius = torch.cat((radius, new_radius), 0)\n","        controls = torch.cat((controls, new_controls), 0)\n","        splits = torch.cat((splits, new_splits), 0)\n","        mask1 = torch.linalg.norm(points, dim = 1, ord = float('inf')) - radius <= R\n","        mask2 = torch.abs(points[:, 0]) - radius <= torch.pi\n","        mask3 = splits < max_splits\n","        mask = mask1 & mask2 & mask3\n","        points = points[mask]\n","        radius = radius[mask]\n","        controls = controls[mask]\n","        splits = splits[mask]\n","\n","\n","  print(time.time() - start)\n","\n","  return verified_points, verified_radius, verified_controls, verified_alphas, verified_indices\n","  #return verified_points, verified_radius, verified_controls, verified_indices\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1750702319994,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"MwZFYagdPzR8"},"outputs":[],"source":["import torch\n","\n","def pad_tensor_rows_1d(tensor, indices, sentinel=sentinel):\n","    \"\"\"\n","    Replace the last (n - k_i) columns of each row in a (m, n) tensor with a sentinel value.\n","\n","    Args:\n","        tensor (torch.Tensor): The input tensor of shape (m, n).\n","        indices (torch.Tensor): A 1D integer tensor of shape (m,), where each value k_i specifies how many\n","                                columns should be kept for row i.\n","        sentinel (float or int): Value to insert in padded positions.\n","\n","    Returns:\n","        torch.Tensor: The modified tensor with sentinel padding applied.\n","    \"\"\"\n","    m, n = tensor.shape\n","\n","    # Generate mask: True where valid, False where to fill with sentinel\n","    column_indices = torch.arange(n, device=tensor.device).expand(m, n)  # (m, n)\n","    mask = column_indices < indices.unsqueeze(1)  # (m, n)\n","\n","    # Fill invalid entries with sentinel\n","    return tensor.masked_fill(~mask, sentinel)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":49,"status":"ok","timestamp":1750702320062,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"fE1C2Q8NP1KA"},"outputs":[],"source":["from matplotlib.ticker import MultipleLocator, FuncFormatter\n","import numpy as np\n","import torch\n","from matplotlib import pyplot as plt\n","\n","def f_printer(yes_points, yes_radius, d: int = 2, R: float = np.pi):\n","\n","  from matplotlib import pyplot as plt\n","  # Plot the phase portrait using quiver\n","  plt.figure(figsize=(6, 6))\n","  fig, ax = plt.subplots()\n","\n","  def format_func(value, tick_number):\n","      N = int(np.round(value / np.pi))\n","      if N == 0:\n","          return \"0\"\n","      elif N == 1:\n","          return r\"$\\pi$\"\n","      elif N == -1:\n","          return r\"-$\\pi$\"\n","      else:\n","          return r\"${0}\\pi$\".format(N)\n","\n","  ax.xaxis.set_major_locator(MultipleLocator(base=np.pi))\n","  ax.xaxis.set_major_formatter(FuncFormatter(format_func))\n","  ax.yaxis.set_major_locator(MultipleLocator(base=np.pi))\n","  ax.yaxis.set_major_formatter(FuncFormatter(format_func))\n","\n","  import matplotlib.pyplot as plt\n","\n","\n","  i = 0\n","  print(len(yes_points))\n","  # Plot each square\n","  for (x, y), r in zip(yes_points, yes_radius):\n","      if i % 5000 == 0:\n","        print(i)\n","      # Create a square patch centered at (x, y) with side length = 2*r\n","      square = plt.Rectangle((x - r, y - r), 2 * r, 2 * r, color='blue', alpha=0.5)\n","      ax.add_patch(square)\n","      i+=1\n","\n","\n","  plt.xlabel('x')\n","  plt.ylabel('y')\n","  plt.grid(True)\n","\n","  plt.xlim(-R, R)\n","  plt.ylim(-R, R)\n","\n","  plt.show()\n","\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":94,"status":"ok","timestamp":1750702320154,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"ogZRSgV1P6Os"},"outputs":[],"source":["import time\n","import torch\n","\n","def interpolate_trajectory_pendulum(points, centers, radii, controls, indices, precision: float = 0.03, target=torch.tensor([0, 0])):\n","    start = time.time()\n","    target_seed = target\n","    found = torch.zeros((points.shape[0],), dtype=torch.bool, device=points.device)\n","    frozen_points = points.clone()\n","    i = 0\n","    counts = torch.zeros(points.shape[0], device=points.device)\n","    meandists = []\n","    maxdists = []\n","\n","    print(\"Iteration \" + str(i) + \":\")\n","    dmax = torch.max(torch.linalg.norm(points, ord=float('inf'), dim=1)).item()\n","    dmean = torch.mean(torch.linalg.norm(points, ord=float('inf'), dim=1)).item()\n","    meandists.append(dmean)\n","    maxdists.append(dmax)\n","    print(\"Maximal Distance from the Origin: \" + str(dmax))\n","    print(\"Average Distance from the Origin: \" + str(dmean))\n","\n","    trajectory = None\n","    end_controls = None\n","    overall_points = points.clone()\n","    failed_points = torch.zeros(1, points.shape[1], device=points.device)\n","\n","    while not torch.all(found):\n","        points = wrap_to_pi(points)\n","        print(len(found))\n","        i += 1\n","        env = PendulumEnv(num_envs=points.shape[0], dimension=points.shape[1])\n","        target = target_seed.repeat(points.shape[0], 1)\n","\n","        # Freeze already found points\n","        current_points = torch.where(found.unsqueeze(1), frozen_points, points)\n","\n","\n","        containing_indices = find_hypercubes(current_points, centers, radii)\n","        mask = containing_indices > -1\n","        failed_points = torch.cat((failed_points, current_points[~mask]), dim=0)\n","\n","        if i == 1:\n","          print(failed_points)\n","\n","        current_points = current_points[mask]\n","        containing_indices = containing_indices[mask]\n","        counts = counts[mask]\n","        target = target[mask]\n","        found = found[mask]\n","        frozen_points = frozen_points[mask]\n","        overall_points = overall_points[mask]\n","        if trajectory is not None:\n","            trajectory = trajectory[mask]\n","        if end_controls is not None:\n","            end_controls = end_controls[mask]\n","\n","        times = indices[containing_indices]\n","        control = controls[containing_indices]\n","        cropped_control = pad_tensor_rows_1d(control, times)\n","        counts += (cropped_control != sentinel).sum(dim=1)\n","        if end_controls is None:\n","            end_controls = cropped_control\n","        else:\n","          end_controls = torch.cat((end_controls, cropped_control), dim=1)\n","\n","        env = PendulumEnv(num_envs = len(current_points), dimension = 2, max_radius=np.pi/2)\n","        env.reset(current_points)\n","        traj = env.trajectories(cropped_control)\n","        traj = wrap_to_pi(traj)\n","\n","        # Replace future trajectory of found points with their frozen position\n","        for j in range(len(found)):\n","            if found[j]:\n","                traj[j, :, :] = frozen_points[j]\n","\n","        # Update frozen status\n","        points = traj[:, -1, :]\n","        mask = torch.linalg.norm(points - target, dim=1, ord=float('inf')) < precision\n","        newly_found = ~found & mask\n","        frozen_points[newly_found] = points[newly_found]\n","        found |= mask\n","\n","        if trajectory is None:\n","            trajectory = traj\n","        else:\n","            trajectory = torch.cat((trajectory, traj[:, 1:, :]), dim=1)\n","            overall_points = torch.cat((overall_points, points), dim=1)\n","\n","        print(\"Iteration \" + str(i) + \":\")\n","        dmax = torch.max(torch.linalg.norm(points, ord=float('inf'), dim=1)).item()\n","        dmean = torch.mean(torch.linalg.norm(points, ord=float('inf'), dim=1)).item()\n","        meandists.append(dmean)\n","        maxdists.append(dmax)\n","        print(\"Maximal Distance from the Origin: \" + str(dmax))\n","        print(\"Average Distance from the Origin: \" + str(dmean))\n","\n","    print((time.time() - start) / points.shape[0])\n","    return trajectory, counts, overall_points, meandists, maxdists, end_controls, failed_points\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":515},"executionInfo":{"elapsed":22381,"status":"error","timestamp":1750702342536,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"qMo_JiUqQCrn","outputId":"ddb3e8cd-2e95-4bdc-c2a1-e2e387977296"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-17-3884723803.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n","  verified_points = torch.tensor([np.zeros(d)]).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["Points:50\n","Points:408\n","Points:497\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-24-2349381643.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRnum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'pi_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_alphas_fixed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mverified_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_controls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_alphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpendulum_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'A103'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;31m#verified_points, verified_radius, verified_controls, verified_indices = pendulum_algo(d = d, epsilon = epsilon, L = L, tau = tau, R = R, batch_size = batch_size, min_alpha = min_alpha, speed = speed, max_splits = i, num_samples=num_samples, dt = dt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-17-3884723803.py\u001b[0m in \u001b[0;36mpendulum_algo\u001b[0;34m(d, R, epsilon, L, tau, min_alpha, batch_size, dt, speed, max_splits, num_samples, function, target, reuse, wrapper)\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0mwinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreusing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindpath_pendulum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_controls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverified_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverified_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_controls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverified_controls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverified_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverified_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverified_radii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_radius\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0mwinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindpath_pendulum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_controls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_radius\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0msol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwinner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-14-2541654831.py\u001b[0m in \u001b[0;36mfindpath_pendulum\u001b[0;34m(env, seeds, countermax, num_samples, variance, time_steps, control_seed, r)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcountermax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Stop when all envs meet countermax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reset start positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         sol_actions, sol_trajectories, sol_distances, taus = env.sample_trajectory(\n\u001b[0m\u001b[1;32m     46\u001b[0m               \u001b[0mtime_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n","\u001b[0;32m/tmp/ipython-input-12-3017487759.py\u001b[0m in \u001b[0;36msample_trajectory\u001b[0;34m(self, time_steps, control_seed, variance, num_samples, r)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mmovement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mbatch_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmovement\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate\u001b[0m  \u001b[0;31m# Move only by instantaneous action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mtrajectories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Store positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import pickle\n","from google.colab import files\n","\n","target_velocity = 0.0\n","\n","d = 2\n","epsilon = 0.01\n","L = 2\n","tau = 3\n","Rnum = 5\n","batch_size = 100\n","min_alpha = 0.0001\n","speed = 0.3\n","num_samples=5000\n","function = inverted_pendulum_2d_torch\n","dt = 0.05\n","save = True\n","R = Rnum * np.pi\n","\n","#for i in range(6):\n","  #print(i)\n","if True:\n","  i = 5\n","  name = function.__name__ + '_' + str(d) + '_' + str(epsilon) + '_' + str(L) + '_' + str(tau) + '_' + str(Rnum) + 'pi_' + str(batch_size) + '_' + str(min_alpha) + '_' + str(speed) + '_' + str(num_samples) + '_' + str(dt) + '_' + str(i) + '_alphas_fixed'\n","  verified_points, verified_radius, verified_controls, verified_alphas, verified_indices = pendulum_algo(d = d, epsilon = epsilon, L = L, tau = tau, R = R, batch_size = batch_size, min_alpha = min_alpha, speed = speed, max_splits = i, num_samples=num_samples, dt = dt, function = function, reuse = False)\n","  name = 'A103'\n","  #verified_points, verified_radius, verified_controls, verified_indices = pendulum_algo(d = d, epsilon = epsilon, L = L, tau = tau, R = R, batch_size = batch_size, min_alpha = min_alpha, speed = speed, max_splits = i, num_samples=num_samples, dt = dt)\n","  if save:\n","    with open(name + \".pkl\", \"wb\") as f:\n","      pickle.dump([verified_points, verified_radius, verified_controls, verified_alphas, verified_indices], f)\n","      #pickle.dump([verified_points, verified_radius, verified_controls, verified_indices], f)\n","    files.download(name + \".pkl\")\n","  if torch.any(torch.abs(verified_points[:, 1] - target_velocity) <= verified_radius):\n","    mi = torch.min((verified_points[:, 0] + verified_radius)[torch.abs(verified_points[:, 1] - target_velocity) <= verified_radius]).cpu().detach().numpy().item()\n","    ma = torch.max((verified_points[:, 0] + verified_radius)[torch.abs(verified_points[:, 1] - target_velocity) <= verified_radius]).cpu().detach().numpy().item()\n","    print((mi, ma))\n","\n","  f_printer(verified_points.detach().to('cpu').numpy(), verified_radius.detach().to('cpu').numpy(), R = (Rnum + 1)*np.pi)\n","\n","\n","  name = function.__name__ + '_' + str(d) + '_' + str(epsilon) + '_' + str(L) + '_' + str(tau) + '_' + str(Rnum) + 'pi_' + str(batch_size) + '_' + str(min_alpha) + '_' + str(speed) + '_' + str(num_samples) + '_' + str(dt) + '_' + str(i) + '_alphas_alt'\n","  name = 'A103'\n","  verified_points_alt, verified_radius_alt, verified_controls_alt, verified_alphas_alt, verified_indices_alt = pendulum_algo_alt(d = d, epsilon = epsilon, L = L, tau = tau, R = R, batch_size = batch_size, min_alpha = min_alpha, speed = speed, max_splits = i, num_samples=num_samples, dt = dt, function = function, reuse = False)\n","  #verified_points, verified_radius, verified_controls, verified_indices = pendulum_algo(d = d, epsilon = epsilon, L = L, tau = tau, R = R, batch_size = batch_size, min_alpha = min_alpha, speed = speed, max_splits = i, num_samples=num_samples, dt = dt)\n","  if save:\n","    with open(name + \".pkl\", \"wb\") as f:\n","      pickle.dump([verified_points_alt, verified_radius_alt, verified_controls_alt, verified_alphas_alt, verified_indices_alt], f)\n","      #pickle.dump([verified_points, verified_radius, verified_controls, verified_indices], f)\n","    files.download(name + \".pkl\")\n","  if torch.any(torch.abs(verified_points[:, 1] - target_velocity) <= verified_radius):\n","    mi = torch.min((verified_points[:, 0] + verified_radius)[torch.abs(verified_points[:, 1] - target_velocity) <= verified_radius]).cpu().detach().numpy().item()\n","    ma = torch.max((verified_points[:, 0] + verified_radius)[torch.abs(verified_points[:, 1] - target_velocity) <= verified_radius]).cpu().detach().numpy().item()\n","    print((mi, ma))\n","\n","  f_printer(verified_points_alt.detach().to('cpu').numpy(), verified_radius_alt.detach().to('cpu').numpy(), R = 4*np.pi)\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"6w1oaMciLqvm","executionInfo":{"status":"error","timestamp":1750720452243,"user_tz":240,"elapsed":18106801,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"df033921-4bfc-4701-c7ae-09cd4c858656"},"outputs":[{"output_type":"stream","name":"stdout","text":["Points:20491\n","Reusing out of Batch:  0 / 10\n","Points:20571\n","Reusing out of Batch:  0 / 10\n","Points:20642\n","Reusing out of Batch:  0 / 10\n","Points:20695\n","Reusing out of Batch:  0 / 10\n","Points:20775\n","Reusing out of Batch:  0 / 10\n","Points:20855\n","Reusing out of Batch:  0 / 10\n","Points:20935\n","Reusing out of Batch:  0 / 10\n","Points:21015\n","Reusing out of Batch:  0 / 10\n","Points:21095\n","Reusing out of Batch:  0 / 10\n","Points:21175\n","Reusing out of Batch:  0 / 10\n","Points:21255\n","Reusing out of Batch:  0 / 10\n","Points:21335\n","Reusing out of Batch:  0 / 10\n","Points:21415\n","Reusing out of Batch:  0 / 10\n","Points:21495\n","Reusing out of Batch:  0 / 10\n","Points:21575\n","Reusing out of Batch:  0 / 10\n","Points:21655\n","Reusing out of Batch:  0 / 10\n","Points:21735\n","Reusing out of Batch:  0 / 10\n","Points:21815\n","Reusing out of Batch:  0 / 10\n","Points:21895\n","Reusing out of Batch:  0 / 10\n","Points:21975\n","Reusing out of Batch:  0 / 10\n","Points:22055\n","Reusing out of Batch:  0 / 10\n","Points:22135\n","Reusing out of Batch:  0 / 10\n","Points:22215\n","Reusing out of Batch:  0 / 10\n","Points:22295\n","Reusing out of Batch:  0 / 10\n","Points:22375\n","Reusing out of Batch:  0 / 10\n","Points:22455\n","Reusing out of Batch:  0 / 10\n","Points:22535\n","Reusing out of Batch:  0 / 10\n","Points:22615\n","Reusing out of Batch:  0 / 10\n","Points:22695\n","Reusing out of Batch:  0 / 10\n","Points:22775\n","Reusing out of Batch:  0 / 10\n","Points:22855\n","Reusing out of Batch:  0 / 10\n","Points:22935\n","Reusing out of Batch:  0 / 10\n","Points:23015\n","Reusing out of Batch:  0 / 10\n","Points:23095\n","Reusing out of Batch:  0 / 10\n","Points:23175\n","Reusing out of Batch:  0 / 10\n","Points:23255\n","Reusing out of Batch:  0 / 10\n","Points:23335\n","Reusing out of Batch:  0 / 10\n","Points:23415\n","Reusing out of Batch:  0 / 10\n","Points:23495\n","Reusing out of Batch:  0 / 10\n","Points:23575\n","Reusing out of Batch:  0 / 10\n","Points:23655\n","Reusing out of Batch:  0 / 10\n","Points:23735\n","Reusing out of Batch:  0 / 10\n","Points:23815\n","Reusing out of Batch:  0 / 10\n","Points:23895\n","Reusing out of Batch:  0 / 10\n","Points:23975\n","Reusing out of Batch:  0 / 10\n","Points:24055\n","Reusing out of Batch:  0 / 10\n","Points:24135\n","Reusing out of Batch:  0 / 10\n","Points:24215\n","Reusing out of Batch:  0 / 10\n","Points:24295\n","Reusing out of Batch:  0 / 10\n","Points:24375\n","Reusing out of Batch:  0 / 10\n","Points:24455\n","Reusing out of Batch:  0 / 10\n","Points:24535\n","Reusing out of Batch:  0 / 10\n","Points:24615\n","Reusing out of Batch:  0 / 10\n","Points:24695\n","Reusing out of Batch:  0 / 10\n","Points:24775\n","Reusing out of Batch:  0 / 10\n","Points:24855\n","Reusing out of Batch:  0 / 10\n","Points:24935\n","Reusing out of Batch:  0 / 10\n","Points:25015\n","Reusing out of Batch:  0 / 10\n","Points:25095\n","Reusing out of Batch:  0 / 10\n","Points:25175\n","Reusing out of Batch:  0 / 10\n","Points:25255\n","Reusing out of Batch:  0 / 10\n","Points:25335\n","Reusing out of Batch:  0 / 10\n","Points:25415\n","Reusing out of Batch:  0 / 10\n","Points:25495\n","Reusing out of Batch:  0 / 10\n","Points:25575\n","Reusing out of Batch:  0 / 10\n","Points:25655\n","Reusing out of Batch:  0 / 10\n","Points:25735\n","Reusing out of Batch:  0 / 10\n","Points:25815\n","Reusing out of Batch:  0 / 10\n","Points:25895\n","Reusing out of Batch:  0 / 10\n","Points:25975\n","Reusing out of Batch:  0 / 10\n","Points:26055\n","Reusing out of Batch:  0 / 10\n","Points:26135\n","Reusing out of Batch:  0 / 10\n","Points:26215\n","Reusing out of Batch:  0 / 10\n","Points:26295\n","Reusing out of Batch:  0 / 10\n","Points:26375\n","Reusing out of Batch:  0 / 10\n","Points:26455\n","Reusing out of Batch:  0 / 10\n","Points:26535\n","Reusing out of Batch:  0 / 10\n","Points:26615\n","Reusing out of Batch:  0 / 10\n","Points:26695\n","Reusing out of Batch:  0 / 10\n","Points:26775\n","Reusing out of Batch:  0 / 10\n","Points:26855\n","Reusing out of Batch:  0 / 10\n","Points:26935\n","Reusing out of Batch:  0 / 10\n","Points:27015\n","Reusing out of Batch:  0 / 10\n","Points:27095\n","Reusing out of Batch:  0 / 10\n","Points:27175\n","Reusing out of Batch:  0 / 10\n","Points:27255\n","Reusing out of Batch:  0 / 10\n","Points:27335\n","Reusing out of Batch:  0 / 10\n","Points:27415\n","Reusing out of Batch:  0 / 10\n","Points:27495\n","Reusing out of Batch:  0 / 10\n","Points:27575\n","Reusing out of Batch:  0 / 10\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_1ecc7a8d-20e8-4ef4-b710-7a7962281b2d\", \"timesave_7200.0.pkl\", 7685161)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Points:27655\n","Reusing out of Batch:  0 / 10\n","Points:27735\n","Reusing out of Batch:  0 / 10\n","Points:27815\n","Reusing out of Batch:  0 / 10\n","Points:27895\n","Reusing out of Batch:  0 / 10\n","Points:27975\n","Reusing out of Batch:  0 / 10\n","Points:28055\n","Reusing out of Batch:  0 / 10\n","Points:28135\n","Reusing out of Batch:  0 / 10\n","Points:28215\n","Reusing out of Batch:  0 / 10\n","Points:28295\n","Reusing out of Batch:  0 / 10\n","Points:28339\n","Reusing out of Batch:  0 / 10\n","Points:28329\n","Reusing out of Batch:  0 / 10\n","Points:28373\n","Reusing out of Batch:  0 / 10\n","Points:28390\n","Reusing out of Batch:  2 / 10\n","Points:28380\n","Reusing out of Batch:  0 / 10\n","Points:28433\n","Reusing out of Batch:  0 / 10\n","Points:28423\n","Reusing out of Batch:  4 / 10\n","Points:28431\n","Reusing out of Batch:  0 / 10\n","Points:28511\n","Reusing out of Batch:  0 / 10\n","Points:28591\n","Reusing out of Batch:  0 / 10\n","Points:28671\n","Reusing out of Batch:  0 / 10\n","Points:28751\n","Reusing out of Batch:  0 / 10\n","Points:28831\n","Reusing out of Batch:  0 / 10\n","Points:28911\n","Reusing out of Batch:  0 / 10\n","Points:28991\n","Reusing out of Batch:  0 / 10\n","Points:29071\n","Reusing out of Batch:  0 / 10\n","Points:29151\n","Reusing out of Batch:  0 / 10\n","Points:29231\n","Reusing out of Batch:  0 / 10\n","Points:29311\n","Reusing out of Batch:  0 / 10\n","Points:29391\n","Reusing out of Batch:  0 / 10\n","Points:29471\n","Reusing out of Batch:  0 / 10\n","Points:29551\n","Reusing out of Batch:  0 / 10\n","Points:29631\n","Reusing out of Batch:  0 / 10\n","Points:29711\n","Reusing out of Batch:  0 / 10\n","Points:29755\n","Reusing out of Batch:  0 / 10\n","Points:29745\n","Reusing out of Batch:  3 / 10\n","Points:29753\n","Reusing out of Batch:  0 / 10\n","Points:29770\n","Reusing out of Batch:  0 / 10\n","Points:29760\n","Reusing out of Batch:  0 / 10\n","Points:29768\n","Reusing out of Batch:  3 / 10\n","Points:29767\n","Reusing out of Batch:  3 / 10\n","Points:29757\n","Reusing out of Batch:  0 / 10\n","Points:29828\n","Reusing out of Batch:  0 / 10\n","Points:29908\n","Reusing out of Batch:  0 / 10\n","Points:29988\n","Reusing out of Batch:  0 / 10\n","Points:30068\n","Reusing out of Batch:  0 / 10\n","Points:30148\n","Reusing out of Batch:  0 / 10\n","Points:30228\n","Reusing out of Batch:  0 / 10\n","Points:30308\n","Reusing out of Batch:  0 / 10\n","Points:30388\n","Reusing out of Batch:  0 / 10\n","Points:30468\n","Reusing out of Batch:  0 / 10\n","Points:30548\n","Reusing out of Batch:  0 / 10\n","Points:30628\n","Reusing out of Batch:  0 / 10\n","Points:30708\n","Reusing out of Batch:  0 / 10\n","Points:30788\n","Reusing out of Batch:  0 / 10\n","Points:30868\n","Reusing out of Batch:  0 / 10\n","Points:30948\n","Reusing out of Batch:  0 / 10\n","Points:31010\n","Reusing out of Batch:  1 / 10\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_ef4a5780-cc47-4403-9945-2e8528329e9d\", \"timesave_9000.0.pkl\", 8584297)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Points:31027\n","Reusing out of Batch:  3 / 10\n","Points:31017\n","Reusing out of Batch:  6 / 10\n","Points:31007\n","Reusing out of Batch:  0 / 10\n","Points:30997\n","Reusing out of Batch:  8 / 10\n","Points:30987\n","Reusing out of Batch:  10 / 10\n","Points:30977\n","Reusing out of Batch:  10 / 10\n","Points:30967\n","Reusing out of Batch:  10 / 10\n","Points:30957\n","Reusing out of Batch:  4 / 10\n","Points:31001\n","Reusing out of Batch:  0 / 10\n","Points:31081\n","Reusing out of Batch:  0 / 10\n","Points:31161\n","Reusing out of Batch:  0 / 10\n","Points:31241\n","Reusing out of Batch:  0 / 10\n","Points:31321\n","Reusing out of Batch:  0 / 10\n","Points:31401\n","Reusing out of Batch:  0 / 10\n","Points:31454\n","Reusing out of Batch:  0 / 10\n","Points:31480\n","Reusing out of Batch:  0 / 10\n","Points:31515\n","Reusing out of Batch:  0 / 10\n","Points:31595\n","Reusing out of Batch:  0 / 10\n","Points:31675\n","Reusing out of Batch:  0 / 10\n","Points:31755\n","Reusing out of Batch:  0 / 10\n","Points:31835\n","Reusing out of Batch:  0 / 10\n","Points:31915\n","Reusing out of Batch:  0 / 10\n","Points:31941\n","Reusing out of Batch:  0 / 10\n","Points:31931\n","Reusing out of Batch:  0 / 10\n","Points:31957\n","Reusing out of Batch:  0 / 10\n","Points:32037\n","Reusing out of Batch:  0 / 10\n","Points:32117\n","Reusing out of Batch:  0 / 10\n","Points:32197\n","Reusing out of Batch:  0 / 10\n","Points:32268\n","Reusing out of Batch:  0 / 10\n","Points:32312\n","Reusing out of Batch:  0 / 10\n","Points:32302\n","Reusing out of Batch:  0 / 10\n","Points:32292\n","Reusing out of Batch:  4 / 10\n","Points:32282\n","Reusing out of Batch:  0 / 10\n","Points:32272\n","Reusing out of Batch:  0 / 10\n","Points:32262\n","Reusing out of Batch:  0 / 10\n","Points:32252\n","Reusing out of Batch:  0 / 10\n","Points:32242\n","Reusing out of Batch:  0 / 10\n","Points:32232\n","Reusing out of Batch:  0 / 10\n","Points:32222\n","Reusing out of Batch:  0 / 10\n","Points:32212\n","Reusing out of Batch:  0 / 10\n","Points:32202\n","Reusing out of Batch:  0 / 10\n","Points:32192\n","Reusing out of Batch:  0 / 10\n","Points:32182\n","Reusing out of Batch:  0 / 10\n","Points:32172\n","Reusing out of Batch:  0 / 10\n","Points:32162\n","Reusing out of Batch:  0 / 10\n","Points:32152\n","Reusing out of Batch:  0 / 10\n","Points:32142\n","Reusing out of Batch:  8 / 10\n","Points:32132\n","Reusing out of Batch:  0 / 10\n","Points:32122\n","Reusing out of Batch:  2 / 10\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_d8922625-4818-49f1-b6be-8d08d4775e35\", \"timesave_10800.0.pkl\", 8998939)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Points:32112\n","Reusing out of Batch:  6 / 10\n","Points:32102\n","Reusing out of Batch:  0 / 10\n","Points:32092\n","Reusing out of Batch:  0 / 10\n","Points:32082\n","Reusing out of Batch:  0 / 10\n","Points:32072\n","Reusing out of Batch:  0 / 10\n","Points:32062\n","Reusing out of Batch:  0 / 10\n","Points:32052\n","Reusing out of Batch:  0 / 10\n","Points:32042\n","Reusing out of Batch:  0 / 10\n","Points:32032\n","Reusing out of Batch:  0 / 10\n","Points:32022\n","Reusing out of Batch:  0 / 10\n","Points:32012\n","Reusing out of Batch:  0 / 10\n","Points:32002\n","Reusing out of Batch:  0 / 10\n","Points:31992\n","Reusing out of Batch:  0 / 10\n","Points:31982\n","Reusing out of Batch:  0 / 10\n","Points:31972\n","Reusing out of Batch:  0 / 10\n","Points:31962\n","Reusing out of Batch:  4 / 10\n","Points:31952\n","Reusing out of Batch:  0 / 10\n","Points:31942\n","Reusing out of Batch:  0 / 10\n","Points:31932\n","Reusing out of Batch:  0 / 10\n","Points:31985\n","Reusing out of Batch:  0 / 10\n","Points:32065\n","Reusing out of Batch:  0 / 10\n","Points:32145\n","Reusing out of Batch:  0 / 10\n","Points:32225\n","Reusing out of Batch:  0 / 10\n","Points:32305\n","Reusing out of Batch:  0 / 10\n","Points:32313\n","Reusing out of Batch:  0 / 10\n","Points:32321\n","Reusing out of Batch:  0 / 10\n","Points:32347\n","Reusing out of Batch:  0 / 10\n","Points:32427\n","Reusing out of Batch:  0 / 10\n","Points:32507\n","Reusing out of Batch:  0 / 10\n","Points:32587\n","Reusing out of Batch:  0 / 10\n","Points:32667\n","Reusing out of Batch:  0 / 10\n","Points:32747\n","Reusing out of Batch:  0 / 10\n","Points:32773\n","Reusing out of Batch:  0 / 10\n","Points:32808\n","Reusing out of Batch:  0 / 10\n","Points:32861\n","Reusing out of Batch:  0 / 10\n","Points:32941\n","Reusing out of Batch:  0 / 10\n","Points:33021\n","Reusing out of Batch:  0 / 10\n","Points:33101\n","Reusing out of Batch:  0 / 10\n","Points:33181\n","Reusing out of Batch:  0 / 10\n","Points:33261\n","Reusing out of Batch:  0 / 10\n","Points:33287\n","Reusing out of Batch:  0 / 10\n","Points:33277\n","Reusing out of Batch:  8 / 10\n","Points:33267\n","Reusing out of Batch:  8 / 10\n","Points:33257\n","Reusing out of Batch:  10 / 10\n","Points:33247\n","Reusing out of Batch:  4 / 10\n","Points:33237\n","Reusing out of Batch:  10 / 10\n","Points:33227\n","Reusing out of Batch:  6 / 10\n","Points:33217\n","Reusing out of Batch:  1 / 10\n","Points:33243\n","Reusing out of Batch:  0 / 10\n","Points:33314\n","Reusing out of Batch:  0 / 10\n","Points:33394\n","Reusing out of Batch:  0 / 10\n","Points:33474\n","Reusing out of Batch:  0 / 10\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_eaf8837b-ef1c-4fcc-9041-68e8097fff49\", \"timesave_12600.0.pkl\", 9399033)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Points:33554\n","Reusing out of Batch:  0 / 10\n","Points:33634\n","Reusing out of Batch:  0 / 10\n","Points:33714\n","Reusing out of Batch:  0 / 10\n","Points:33794\n","Reusing out of Batch:  0 / 10\n","Points:33874\n","Reusing out of Batch:  0 / 10\n","Points:33954\n","Reusing out of Batch:  0 / 10\n","Points:34034\n","Reusing out of Batch:  0 / 10\n","Points:34114\n","Reusing out of Batch:  0 / 10\n","Points:34194\n","Reusing out of Batch:  0 / 10\n","Points:34274\n","Reusing out of Batch:  0 / 10\n","Points:34354\n","Reusing out of Batch:  0 / 10\n","Points:34434\n","Reusing out of Batch:  3 / 10\n","Points:34487\n","Reusing out of Batch:  10 / 10\n","Points:34477\n","Reusing out of Batch:  1 / 10\n","Points:34476\n","Reusing out of Batch:  6 / 10\n","Points:34484\n","Reusing out of Batch:  6 / 10\n","Points:34474\n","Reusing out of Batch:  0 / 10\n","Points:34500\n","Reusing out of Batch:  7 / 10\n","Points:34499\n","Reusing out of Batch:  1 / 10\n","Points:34489\n","Reusing out of Batch:  0 / 10\n","Points:34551\n","Reusing out of Batch:  0 / 10\n","Points:34631\n","Reusing out of Batch:  0 / 10\n","Points:34711\n","Reusing out of Batch:  0 / 10\n","Points:34791\n","Reusing out of Batch:  0 / 10\n","Points:34871\n","Reusing out of Batch:  0 / 10\n","Points:34951\n","Reusing out of Batch:  0 / 10\n","Points:35031\n","Reusing out of Batch:  0 / 10\n","Points:35111\n","Reusing out of Batch:  0 / 10\n","Points:35191\n","Reusing out of Batch:  0 / 10\n","Points:35271\n","Reusing out of Batch:  0 / 10\n","Points:35351\n","Reusing out of Batch:  0 / 10\n","Points:35431\n","Reusing out of Batch:  0 / 10\n","Points:35511\n","Reusing out of Batch:  0 / 10\n","Points:35591\n","Reusing out of Batch:  0 / 10\n","Points:35671\n","Reusing out of Batch:  0 / 10\n","Points:35751\n","Reusing out of Batch:  0 / 10\n","Points:35831\n","Reusing out of Batch:  6 / 10\n","Points:35821\n","Reusing out of Batch:  0 / 10\n","Points:35820\n","Reusing out of Batch:  2 / 10\n","Points:35864\n","Reusing out of Batch:  1 / 10\n","Points:35854\n","Reusing out of Batch:  0 / 10\n","Points:35889\n","Reusing out of Batch:  3 / 10\n","Points:35915\n","Reusing out of Batch:  0 / 10\n","Points:35905\n","Reusing out of Batch:  0 / 10\n","Points:35967\n","Reusing out of Batch:  0 / 10\n","Points:36047\n","Reusing out of Batch:  0 / 10\n","Points:36127\n","Reusing out of Batch:  0 / 10\n","Points:36207\n","Reusing out of Batch:  0 / 10\n","Points:36287\n","Reusing out of Batch:  0 / 10\n","Points:36367\n","Reusing out of Batch:  0 / 10\n","Points:36447\n","Reusing out of Batch:  0 / 10\n","Points:36527\n","Reusing out of Batch:  0 / 10\n","Points:36607\n","Reusing out of Batch:  0 / 10\n","Points:36687\n","Reusing out of Batch:  0 / 10\n","Points:36767\n","Reusing out of Batch:  0 / 10\n","Points:36847\n","Reusing out of Batch:  0 / 10\n","Points:36927\n","Reusing out of Batch:  0 / 10\n","Points:37007\n","Reusing out of Batch:  0 / 10\n","Points:37087\n","Reusing out of Batch:  0 / 10\n","Points:37167\n","Reusing out of Batch:  0 / 10\n","Points:37247\n","Reusing out of Batch:  0 / 10\n","Points:37327\n","Reusing out of Batch:  0 / 10\n","Points:37407\n","Reusing out of Batch:  0 / 10\n","Points:37487\n","Reusing out of Batch:  0 / 10\n","Points:37567\n","Reusing out of Batch:  0 / 10\n","Points:37647\n","Reusing out of Batch:  0 / 10\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_b92529c8-a773-4a98-a5f1-9a84ee1aa68a\", \"timesave_14400.0.pkl\", 10500779)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Points:37727\n","Reusing out of Batch:  0 / 10\n","Points:37807\n","Reusing out of Batch:  0 / 10\n","Points:37887\n","Reusing out of Batch:  0 / 10\n","Points:37967\n","Reusing out of Batch:  0 / 10\n","Points:38047\n","Reusing out of Batch:  0 / 10\n","Points:38127\n","Reusing out of Batch:  0 / 10\n","Points:38207\n","Reusing out of Batch:  0 / 10\n","Points:38287\n","Reusing out of Batch:  0 / 10\n","Points:38367\n","Reusing out of Batch:  0 / 10\n","Points:38447\n","Reusing out of Batch:  0 / 10\n","Points:38527\n","Reusing out of Batch:  0 / 10\n","Points:38607\n","Reusing out of Batch:  0 / 10\n","Points:38687\n","Reusing out of Batch:  0 / 10\n","Points:38767\n","Reusing out of Batch:  0 / 10\n","Points:38847\n","Reusing out of Batch:  0 / 10\n","Points:38927\n","Reusing out of Batch:  0 / 10\n","Points:39007\n","Reusing out of Batch:  0 / 10\n","Points:39087\n","Reusing out of Batch:  0 / 10\n","Points:39167\n","Reusing out of Batch:  0 / 10\n","Points:39247\n","Reusing out of Batch:  0 / 10\n","Points:39327\n","Reusing out of Batch:  0 / 10\n","Points:39407\n","Reusing out of Batch:  0 / 10\n","Points:39487\n","Reusing out of Batch:  0 / 10\n","Points:39567\n","Reusing out of Batch:  0 / 10\n","Points:39647\n","Reusing out of Batch:  0 / 10\n","Points:39727\n","Reusing out of Batch:  0 / 10\n","Points:39807\n","Reusing out of Batch:  0 / 10\n","Points:39887\n","Reusing out of Batch:  0 / 10\n","Points:39967\n","Reusing out of Batch:  0 / 10\n","Points:40047\n","Reusing out of Batch:  0 / 10\n","Points:40127\n","Reusing out of Batch:  0 / 10\n","Points:40207\n","Reusing out of Batch:  0 / 10\n","Points:40287\n","Reusing out of Batch:  0 / 10\n","Points:40367\n","Reusing out of Batch:  0 / 10\n","Points:40447\n","Reusing out of Batch:  0 / 10\n","Points:40527\n","Reusing out of Batch:  0 / 10\n","Points:40607\n","Reusing out of Batch:  0 / 10\n","Points:40687\n","Reusing out of Batch:  0 / 10\n","Points:40767\n","Reusing out of Batch:  0 / 10\n","Points:40847\n","Reusing out of Batch:  0 / 10\n","Points:40927\n","Reusing out of Batch:  0 / 10\n","Points:41007\n","Reusing out of Batch:  0 / 10\n","Points:41087\n","Reusing out of Batch:  0 / 10\n","Points:41167\n","Reusing out of Batch:  0 / 10\n","Points:41247\n","Reusing out of Batch:  0 / 10\n","Points:41327\n","Reusing out of Batch:  0 / 10\n","Points:41407\n","Reusing out of Batch:  0 / 10\n","Points:41487\n","Reusing out of Batch:  0 / 10\n","Points:41567\n","Reusing out of Batch:  0 / 10\n","Points:41647\n","Reusing out of Batch:  0 / 10\n","Points:41727\n","Reusing out of Batch:  0 / 10\n","Points:41807\n","Reusing out of Batch:  0 / 10\n","Points:41887\n","Reusing out of Batch:  0 / 10\n","Points:41967\n","Reusing out of Batch:  0 / 10\n","Points:42047\n","Reusing out of Batch:  0 / 10\n","Points:42127\n","Reusing out of Batch:  0 / 10\n","Points:42207\n","Reusing out of Batch:  0 / 10\n","Points:42287\n","Reusing out of Batch:  0 / 10\n","Points:42367\n","Reusing out of Batch:  0 / 10\n","Points:42447\n","Reusing out of Batch:  0 / 10\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_259ff341-4587-4a9c-8826-48b112ddbf3b\", \"timesave_16200.0.pkl\", 11729577)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Points:42527\n","Reusing out of Batch:  0 / 10\n","Points:42607\n","Reusing out of Batch:  0 / 10\n","Points:42687\n","Reusing out of Batch:  0 / 10\n","Points:42767\n","Reusing out of Batch:  0 / 10\n","Points:42847\n","Reusing out of Batch:  0 / 10\n","Points:42927\n","Reusing out of Batch:  0 / 10\n","Points:43007\n","Reusing out of Batch:  0 / 10\n","Points:43087\n","Reusing out of Batch:  3 / 10\n","Points:43095\n","Reusing out of Batch:  10 / 10\n","Points:43085\n","Reusing out of Batch:  9 / 10\n","Points:43075\n","Reusing out of Batch:  6 / 10\n","Points:43065\n","Reusing out of Batch:  10 / 10\n","Points:43055\n","Reusing out of Batch:  7 / 10\n","Points:43045\n","Reusing out of Batch:  7 / 10\n","Points:43044\n","Reusing out of Batch:  10 / 10\n","Points:43034\n","Reusing out of Batch:  10 / 10\n","Points:43024\n","Reusing out of Batch:  9 / 10\n","Points:43014\n","Reusing out of Batch:  8 / 10\n","Points:43004\n","Reusing out of Batch:  10 / 10\n","Points:42994\n","Reusing out of Batch:  10 / 10\n","Points:42984\n","Reusing out of Batch:  10 / 10\n","Points:42974\n","Reusing out of Batch:  10 / 10\n","Points:42964\n","Reusing out of Batch:  10 / 10\n","Points:42954\n","Reusing out of Batch:  9 / 10\n","Points:42953\n","Reusing out of Batch:  4 / 10\n","Points:42997\n","Reusing out of Batch:  0 / 10\n","Points:43077\n","Reusing out of Batch:  9 / 10\n","Points:43076\n","Reusing out of Batch:  3 / 10\n","Points:43129\n","Reusing out of Batch:  2 / 10\n","Points:43191\n","Reusing out of Batch:  10 / 10\n","Points:43181\n","Reusing out of Batch:  7 / 10\n","Points:43198\n","Reusing out of Batch:  3 / 10\n","Points:43251\n","Reusing out of Batch:  7 / 10\n","Points:43268\n","Reusing out of Batch:  10 / 10\n","Points:43258\n","Reusing out of Batch:  2 / 10\n","Points:43320\n","Reusing out of Batch:  7 / 10\n","Points:43337\n","Reusing out of Batch:  9 / 10\n","Points:43336\n","Reusing out of Batch:  0 / 10\n","Points:43416\n","Reusing out of Batch:  6 / 10\n","Points:43442\n","Reusing out of Batch:  10 / 10\n","Points:43432\n","Reusing out of Batch:  10 / 10\n","Points:43422\n","Reusing out of Batch:  10 / 10\n","Points:43412\n","Reusing out of Batch:  10 / 10\n","Points:43402\n","Reusing out of Batch:  10 / 10\n","Points:43392\n","Reusing out of Batch:  10 / 10\n","Points:43382\n","Reusing out of Batch:  10 / 10\n","Points:43372\n","Reusing out of Batch:  10 / 10\n","Points:43362\n","Reusing out of Batch:  10 / 10\n","Points:43352\n","Reusing out of Batch:  10 / 10\n","Points:43342\n","Reusing out of Batch:  9 / 10\n","Points:43341\n","Reusing out of Batch:  10 / 10\n","Points:43331\n","Reusing out of Batch:  10 / 10\n","Points:43321\n","Reusing out of Batch:  10 / 10\n","Points:43311\n","Reusing out of Batch:  10 / 10\n","Points:43301\n","Reusing out of Batch:  10 / 10\n","Points:43291\n","Reusing out of Batch:  8 / 10\n","Points:43299\n","Reusing out of Batch:  0 / 10\n","Points:43379\n","Reusing out of Batch:  4 / 10\n","Points:43423\n","Reusing out of Batch:  4 / 10\n","Points:43467\n","Reusing out of Batch:  0 / 10\n","Points:43547\n","Reusing out of Batch:  2 / 10\n","Points:43609\n","Reusing out of Batch:  1 / 10\n","Points:43680\n","Reusing out of Batch:  0 / 10\n","Points:43760\n","Reusing out of Batch:  1 / 10\n","Points:43831\n","Reusing out of Batch:  10 / 10\n","Points:43821\n","Reusing out of Batch:  10 / 10\n","Points:43811\n","Reusing out of Batch:  10 / 10\n","Points:43801\n","Reusing out of Batch:  10 / 10\n","Points:43791\n","Reusing out of Batch:  10 / 10\n","Points:43781\n","Reusing out of Batch:  10 / 10\n","Points:43771\n","Reusing out of Batch:  10 / 10\n","Points:43761\n","Reusing out of Batch:  10 / 10\n","Points:43751\n","Reusing out of Batch:  10 / 10\n","Points:43741\n","Reusing out of Batch:  10 / 10\n","Points:43731\n","Reusing out of Batch:  10 / 10\n","Points:43721\n","Reusing out of Batch:  10 / 10\n","Points:43711\n","Reusing out of Batch:  10 / 10\n","Points:43701\n","Reusing out of Batch:  10 / 10\n","Points:43691\n","Reusing out of Batch:  10 / 10\n","Points:43681\n","Reusing out of Batch:  10 / 10\n","Points:43671\n","Reusing out of Batch:  1 / 10\n","Points:43742\n","Reusing out of Batch:  0 / 10\n","Points:43822\n","Reusing out of Batch:  0 / 10\n","Points:43902\n","Reusing out of Batch:  0 / 10\n","Points:43982\n","Reusing out of Batch:  0 / 10\n","Points:44062\n","Reusing out of Batch:  0 / 10\n","Points:44142\n","Reusing out of Batch:  0 / 10\n","Points:44222\n","Reusing out of Batch:  0 / 10\n","Points:44302\n","Reusing out of Batch:  0 / 10\n","Points:44382\n","Reusing out of Batch:  0 / 10\n","Points:44462\n","Reusing out of Batch:  0 / 10\n","Points:44542\n","Reusing out of Batch:  0 / 10\n","Points:44622\n","Reusing out of Batch:  0 / 10\n","Points:44702\n","Reusing out of Batch:  0 / 10\n","Points:44782\n","Reusing out of Batch:  0 / 10\n","Points:44862\n","Reusing out of Batch:  0 / 10\n","Points:44942\n","Reusing out of Batch:  0 / 10\n","Points:44986\n","Reusing out of Batch:  1 / 10\n","Points:44976\n","Reusing out of Batch:  1 / 10\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_19097bf5-0731-48a7-8931-a6ddcae30be4\", \"timesave_18000.0.pkl\", 12524741)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Points:44966\n","Reusing out of Batch:  0 / 10\n","Points:44992\n","Reusing out of Batch:  0 / 10\n","Points:44982\n","Reusing out of Batch:  0 / 10\n","Points:44990\n","Reusing out of Batch:  0 / 10\n","Points:45016\n","Reusing out of Batch:  0 / 10\n","Points:45006\n","Reusing out of Batch:  1 / 10\n","Points:45041\n","Reusing out of Batch:  2 / 10\n","Points:45103\n","Reusing out of Batch:  0 / 10\n","Points:45183\n","Reusing out of Batch:  9 / 10\n","Points:45182\n","Reusing out of Batch:  3 / 10\n","Points:45235\n","Reusing out of Batch:  2 / 10\n","Points:45297\n","Reusing out of Batch:  10 / 10\n","Points:45287\n","Reusing out of Batch:  8 / 10\n","Points:45295\n","Reusing out of Batch:  5 / 10\n","Points:45330\n","Reusing out of Batch:  0 / 10\n","Points:45410\n","Reusing out of Batch:  0 / 10\n","Points:45490\n","Reusing out of Batch:  0 / 10\n","Points:45570\n","Reusing out of Batch:  0 / 10\n","Points:45650\n","Reusing out of Batch:  0 / 10\n","Points:45730\n","Reusing out of Batch:  3 / 10\n","Points:45783\n","Reusing out of Batch:  0 / 10\n","Points:45863\n","Reusing out of Batch:  0 / 10\n","Points:45934\n","Reusing out of Batch:  0 / 10\n","Points:45951\n","Reusing out of Batch:  0 / 10\n","Points:45941\n","Reusing out of Batch:  0 / 10\n","Points:45976\n","Reusing out of Batch:  0 / 10\n","Points:45975\n","Reusing out of Batch:  1 / 10\n","Points:45965\n","Reusing out of Batch:  0 / 10\n","Points:46009\n","Reusing out of Batch:  0 / 10\n","Points:45999\n","Reusing out of Batch:  3 / 10\n","Points:45989\n","Reusing out of Batch:  10 / 10\n","Points:45979\n","Reusing out of Batch:  10 / 10\n","Points:45969\n","Reusing out of Batch:  10 / 10\n","Points:45959\n","Reusing out of Batch:  10 / 10\n","Points:45949\n","Reusing out of Batch:  10 / 10\n","Points:45939\n","Reusing out of Batch:  10 / 10\n","Points:45929\n","Reusing out of Batch:  10 / 10\n","Points:45919\n","Reusing out of Batch:  10 / 10\n","Points:45909\n","Reusing out of Batch:  10 / 10\n","Points:45899\n","Reusing out of Batch:  10 / 10\n","Points:45889\n","Reusing out of Batch:  10 / 10\n","Points:45879\n","Reusing out of Batch:  10 / 10\n","Points:45869\n","Reusing out of Batch:  10 / 10\n","Points:45859\n","Reusing out of Batch:  10 / 10\n","Points:45849\n","Reusing out of Batch:  10 / 10\n","Points:45839\n","Reusing out of Batch:  10 / 10\n","Points:45829\n","Reusing out of Batch:  10 / 10\n","Points:45819\n","Reusing out of Batch:  10 / 10\n","Points:45809\n","Reusing out of Batch:  9 / 10\n","Points:45799\n","Reusing out of Batch:  10 / 10\n","Points:45789\n","Reusing out of Batch:  5 / 10\n","Points:45779\n","Reusing out of Batch:  10 / 10\n","Points:45769\n","Reusing out of Batch:  10 / 10\n","Points:45759\n","Reusing out of Batch:  10 / 10\n","Points:45749\n","Reusing out of Batch:  0 / 10\n","Points:45748\n","Reusing out of Batch:  0 / 10\n","Points:45828\n","Reusing out of Batch:  0 / 10\n","Points:45908\n","Reusing out of Batch:  0 / 10\n","Points:45988\n","Reusing out of Batch:  0 / 10\n","Points:46068\n","Reusing out of Batch:  0 / 10\n","Points:46148\n","Reusing out of Batch:  0 / 10\n","Points:46228\n","Reusing out of Batch:  0 / 10\n","Points:46308\n","Reusing out of Batch:  0 / 10\n","Points:46388\n","Reusing out of Batch:  7 / 10\n","Points:46405\n","Reusing out of Batch:  10 / 10\n","Points:46395\n","Reusing out of Batch:  8 / 10\n","Points:46403\n","Reusing out of Batch:  6 / 10\n","Points:46429\n","Reusing out of Batch:  10 / 10\n","Points:46419\n","Reusing out of Batch:  5 / 10\n","Points:46454\n","Reusing out of Batch:  7 / 10\n","Points:46471\n","Reusing out of Batch:  10 / 10\n","Points:46461\n","Reusing out of Batch:  10 / 10\n","Points:46451\n","Reusing out of Batch:  10 / 10\n","Points:46441\n","Reusing out of Batch:  10 / 10\n","Points:46431\n","Reusing out of Batch:  10 / 10\n","Points:46421\n","Reusing out of Batch:  10 / 10\n","Points:46411\n","Reusing out of Batch:  10 / 10\n","Points:46401\n","Reusing out of Batch:  10 / 10\n","Points:46391\n","Reusing out of Batch:  10 / 10\n","Points:46381\n","Reusing out of Batch:  2 / 10\n","Points:46443\n","Reusing out of Batch:  0 / 10\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_2552988a-d004-4b47-a8f0-0d88c2853bf8\", \"timesave_19800.0.pkl\", 13066996)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Points:46523\n","Reusing out of Batch:  0 / 10\n","Points:46603\n","Reusing out of Batch:  0 / 10\n","Points:46683\n","Reusing out of Batch:  0 / 10\n","Points:46763\n","Reusing out of Batch:  0 / 10\n","Points:46843\n","Reusing out of Batch:  0 / 10\n","Points:46923\n","Reusing out of Batch:  0 / 10\n","Points:46985\n","Reusing out of Batch:  0 / 10\n","Points:47038\n","Reusing out of Batch:  8 / 10\n","Points:47046\n","Reusing out of Batch:  10 / 10\n","Points:47036\n","Reusing out of Batch:  1 / 10\n","Points:47107\n","Reusing out of Batch:  10 / 10\n","Points:47097\n","Reusing out of Batch:  7 / 10\n","Points:47114\n","Reusing out of Batch:  2 / 10\n","Points:47149\n","Reusing out of Batch:  9 / 10\n","Points:47139\n","Reusing out of Batch:  10 / 10\n","Points:47129\n","Reusing out of Batch:  10 / 10\n","Points:47119\n","Reusing out of Batch:  10 / 10\n","Points:47109\n","Reusing out of Batch:  9 / 10\n","Points:47108\n","Reusing out of Batch:  10 / 10\n","Points:47098\n","Reusing out of Batch:  9 / 10\n","Points:47097\n","Reusing out of Batch:  10 / 10\n","Points:47087\n","Reusing out of Batch:  10 / 10\n","Points:47077\n","Reusing out of Batch:  5 / 10\n","Points:47067\n","Reusing out of Batch:  0 / 10\n","Points:47057\n","Reusing out of Batch:  0 / 10\n","Points:47047\n","Reusing out of Batch:  0 / 10\n","Points:47037\n","Reusing out of Batch:  0 / 10\n","Points:47027\n","Reusing out of Batch:  0 / 10\n","Points:47017\n","Reusing out of Batch:  0 / 10\n","Points:47007\n","Reusing out of Batch:  0 / 10\n","Points:46997\n","Reusing out of Batch:  0 / 10\n","Points:46987\n","Reusing out of Batch:  4 / 10\n","Points:46977\n","Reusing out of Batch:  10 / 10\n","Points:46967\n","Reusing out of Batch:  3 / 10\n","Points:46957\n","Reusing out of Batch:  8 / 10\n","Points:46947\n","Reusing out of Batch:  10 / 10\n","Points:46937\n","Reusing out of Batch:  6 / 10\n","Points:46927\n","Reusing out of Batch:  10 / 10\n","Points:46917\n","Reusing out of Batch:  10 / 10\n","Points:46907\n","Reusing out of Batch:  10 / 10\n","Points:46897\n","Reusing out of Batch:  10 / 10\n","Points:46887\n","Reusing out of Batch:  8 / 10\n","Points:46877\n","Reusing out of Batch:  10 / 10\n","Points:46867\n","Reusing out of Batch:  10 / 10\n","Points:46857\n","Reusing out of Batch:  10 / 10\n","Points:46847\n","Reusing out of Batch:  2 / 10\n","Points:46837\n","Reusing out of Batch:  2 / 10\n","Points:46827\n","Reusing out of Batch:  0 / 10\n","Points:46817\n","Reusing out of Batch:  0 / 10\n","Points:46807\n","Reusing out of Batch:  3 / 10\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_0ef7184e-fd4c-4b55-a3d0-423ec8d4bdb9\", \"timesave_21600.0.pkl\", 13326550)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Points:46797\n","Reusing out of Batch:  0 / 10\n","Points:46787\n","Reusing out of Batch:  0 / 10\n","Points:46777\n","Reusing out of Batch:  0 / 10\n","Points:46767\n","Reusing out of Batch:  0 / 10\n","Points:46757\n","Reusing out of Batch:  0 / 10\n","Points:46747\n","Reusing out of Batch:  0 / 10\n","Points:46737\n","Reusing out of Batch:  0 / 10\n","Points:46727\n","Reusing out of Batch:  0 / 10\n","Points:46717\n","Reusing out of Batch:  0 / 10\n","Points:46707\n","Reusing out of Batch:  0 / 10\n","Points:46697\n","Reusing out of Batch:  0 / 10\n","Points:46687\n","Reusing out of Batch:  0 / 10\n","Points:46677\n","Reusing out of Batch:  0 / 10\n","Points:46667\n","Reusing out of Batch:  0 / 10\n","Points:46657\n","Reusing out of Batch:  0 / 10\n","Points:46647\n","Reusing out of Batch:  0 / 10\n","Points:46637\n","Reusing out of Batch:  0 / 10\n","Points:46627\n","Reusing out of Batch:  0 / 10\n","Points:46626\n","Reusing out of Batch:  0 / 10\n","Points:46697\n","Reusing out of Batch:  0 / 10\n","Points:46777\n","Reusing out of Batch:  0 / 10\n","Points:46857\n","Reusing out of Batch:  0 / 10\n","Points:46937\n","Reusing out of Batch:  0 / 10\n","Points:47017\n","Reusing out of Batch:  0 / 10\n","Points:47079\n","Reusing out of Batch:  0 / 10\n","Points:47096\n","Reusing out of Batch:  0 / 10\n","Points:47140\n","Reusing out of Batch:  0 / 10\n","Points:47220\n","Reusing out of Batch:  0 / 10\n","Points:47300\n","Reusing out of Batch:  0 / 10\n","Points:47380\n","Reusing out of Batch:  0 / 10\n","Points:47460\n","Reusing out of Batch:  0 / 10\n","Points:47540\n","Reusing out of Batch:  0 / 10\n","Points:47611\n","Reusing out of Batch:  0 / 10\n","Points:47655\n","Reusing out of Batch:  0 / 10\n","Points:47735\n","Reusing out of Batch:  0 / 10\n","Points:47815\n","Reusing out of Batch:  0 / 10\n","Points:47895\n","Reusing out of Batch:  0 / 10\n","Points:47975\n","Reusing out of Batch:  0 / 10\n","Points:48055\n","Reusing out of Batch:  0 / 10\n","Points:48135\n","Reusing out of Batch:  0 / 10\n","Points:48215\n","Reusing out of Batch:  0 / 10\n","Points:48295\n","Reusing out of Batch:  0 / 10\n","Points:48375\n","Reusing out of Batch:  0 / 10\n","Points:48455\n","Reusing out of Batch:  0 / 10\n","Points:48535\n","Reusing out of Batch:  0 / 10\n","Points:48615\n","Reusing out of Batch:  0 / 10\n","Points:48695\n","Reusing out of Batch:  0 / 10\n","Points:48775\n","Reusing out of Batch:  0 / 10\n","Points:48855\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 3.64 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.63 GiB is free. Process 2440 has 11.11 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 44.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-25-1769245422.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mverified_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_controls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_alphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverified_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverified_radius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpendulum_algo_reuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavetime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'A141'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-18-1941450801.py\u001b[0m in \u001b[0;36mpendulum_algo_reuse\u001b[0;34m(d, R, epsilon, L, tau, min_alpha, batch_size, dt, speed, max_splits, num_samples, function, target, reuse, wrapper, savetime, cont, conttime, contlist)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mwinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreusing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindpath_pendulum_reuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_controls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverified_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverified_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_alphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverified_alphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_controls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverified_controls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverified_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverified_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverified_radii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_radius\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mwinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindpath_pendulum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_controls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_radius\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-15-2568640269.py\u001b[0m in \u001b[0;36mfindpath_pendulum_reuse\u001b[0;34m(env, seeds, countermax, num_samples, variance, time_steps, control_seed, r, centers, radii, verified_alphas, verified_controls, verified_indices, splits, unverified_centers, unverified_radii)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reset start positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcenters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mradii\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mverified_controls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mverified_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m           sol_actions, sol_trajectories, sol_distances, taus, reusing = env.sample_trajectory_reuse_new(\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mtime_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mradii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_alphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverified_alphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_controls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverified_controls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverified_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverified_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverified_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munverified_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverified_radii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munverified_radii\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m           )\n","\u001b[0;32m/tmp/ipython-input-12-3017487759.py\u001b[0m in \u001b[0;36msample_trajectory_reuse_new\u001b[0;34m(self, time_steps, control_seed, variance, num_samples, r, centers, radii, verified_alphas, verified_controls, verified_indices, splits, unverified_centers, unverified_radii)\u001b[0m\n\u001b[1;32m    368\u001b[0m               \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m               \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m               idx = check_parallel(batch_states[active_indices], t, r[active_indices], self.alpha,\n\u001b[0m\u001b[1;32m    371\u001b[0m                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                                \u001b[0munverified_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverified_radii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-11-1184918128.py\u001b[0m in \u001b[0;36mcheck_parallel\u001b[0;34m(batch_states, t, r, alpha, L, rate, num_samples, unverified_centers, unverified_radii, verified_centers, verified_radii, verified_alphas, verified_indices, trajectories)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Count unverified intersections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     unverified_count = count_hypercube_intersections_wrap(\n\u001b[0m\u001b[1;32m     15\u001b[0m          \u001b[0munverified_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverified_radii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradii_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     )\n","\u001b[0;32m/tmp/ipython-input-9-4036559000.py\u001b[0m in \u001b[0;36mcount_hypercube_intersections_wrap\u001b[0;34m(centers, radii, query_centers, query_radii)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mintersects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mangle_dist\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mangular_thresh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mintersects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (M,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.64 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.63 GiB is free. Process 2440 has 11.11 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 44.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["import pickle\n","from google.colab import files\n","\n","target_velocity = 0.0\n","\n","d = 2\n","epsilon = 0.01\n","L = 2\n","tau = 3\n","Rnum = 5\n","batch_size = 10\n","min_alpha = 0.0001\n","speed = 0.3\n","num_samples=1000\n","function = inverted_pendulum_2d_torch\n","dt = 0.05\n","save = True\n","R = Rnum * np.pi\n","\n","#for i in range(6):\n","  #print(i)\n","if True:\n","  i = 5\n","  verified_points, verified_radius, verified_controls, verified_alphas, verified_indices, unverified_points, unverified_radius = pendulum_algo_reuse(d = d, epsilon = epsilon, L = L, tau = tau, R = R, batch_size = batch_size, min_alpha = min_alpha, speed = speed, max_splits = i, num_samples=num_samples, dt = dt, function = function, reuse = True, savetime = 1800, cont = True, conttime = 5400)\n","  name = 'A141'\n","  if save:\n","    with open(name + \".pkl\", \"wb\") as f:\n","      pickle.dump([verified_points, verified_radius, verified_controls, verified_alphas, verified_indices, unverified_points, unverified_radius], f)\n","      #pickle.dump([verified_points, verified_radius, verified_controls, verified_indices], f)\n","    files.download(name + \".pkl\")\n","\n","  f_printer(verified_points.detach().to('cpu').numpy(), verified_radius.detach().to('cpu').numpy(), R = (Rnum + 1)*np.pi)\n","\n","\n","\n","  batch_size = 250\n","  tau = 1\n","  verified_points, verified_radius, verified_controls, verified_alphas, verified_indices = pendulum_algo(d = d, epsilon = epsilon, L = L, tau = tau, R = R, batch_size = batch_size, min_alpha = min_alpha, speed = speed, max_splits = i, num_samples=num_samples, dt = dt, function = function, reuse = True)\n","  name = 'A132'\n","  if save:\n","    with open(name + \".pkl\", \"wb\") as f:\n","      pickle.dump([verified_points, verified_radius, verified_controls, verified_alphas, verified_indices], f)\n","      #pickle.dump([verified_points, verified_radius, verified_controls, verified_indices], f)\n","    files.download(name + \".pkl\")\n","\n","  f_printer(verified_points.detach().to('cpu').numpy(), verified_radius.detach().to('cpu').numpy(), R = (Rnum + 1)*np.pi)\n","\n","\n","\n","  batch_size = 100\n","  tau = 2\n","  verified_points, verified_radius, verified_controls, verified_alphas, verified_indices = pendulum_algo(d = d, epsilon = epsilon, L = L, tau = tau, R = R, batch_size = batch_size, min_alpha = min_alpha, speed = speed, max_splits = i, num_samples=num_samples, dt = dt, function = function, reuse = True)\n","  name = 'A133'\n","  if save:\n","    with open(name + \".pkl\", \"wb\") as f:\n","      pickle.dump([verified_points, verified_radius, verified_controls, verified_alphas, verified_indices], f)\n","      #pickle.dump([verified_points, verified_radius, verified_controls, verified_indices], f)\n","    files.download(name + \".pkl\")\n","\n","  f_printer(verified_points.detach().to('cpu').numpy(), verified_radius.detach().to('cpu').numpy(), R = (Rnum + 1)*np.pi)"]},{"cell_type":"code","source":["import pickle\n","from google.colab import files\n","\n","target_velocity = 0.0\n","\n","d = 2\n","epsilon = 0.01\n","L = 5\n","tau = 2\n","Rnum = 5\n","batch_size = 8\n","min_alpha = 0.0001\n","speed = 0.4\n","num_samples=1000\n","function = simplified_pendulum_derivatives\n","dt = 0.05\n","save = True\n","R = Rnum * np.pi\n","\n","#for i in range(6):\n","  #print(i)\n","if True:\n","  i = 5\n","  verified_points, verified_radius, verified_controls, verified_alphas, verified_indices, unverified_points, unverified_radius = pendulum_algo_reuse(d = d, epsilon = epsilon, L = L, tau = tau, R = R, batch_size = batch_size, min_alpha = min_alpha, speed = speed, max_splits = i, num_samples=num_samples, dt = dt, function = function, reuse = True, savetime = 1800, cont = True, conttime = None, contlist = [verified_points, verified_radius, verified_controls, verified_alphas, verified_indices, unverified_points, unverified_radius])\n","  name = 'A132'\n","  if save:\n","    with open(name + \".pkl\", \"wb\") as f:\n","      pickle.dump([verified_points, verified_radius, verified_controls, verified_alphas, verified_indices, unverified_points, unverified_radius], f)\n","      #pickle.dump([verified_points, verified_radius, verified_controls, verified_indices], f)\n","    files.download(name + \".pkl\")\n","\n","  f_printer(verified_points.detach().to('cpu').numpy(), verified_radius.detach().to('cpu').numpy(), R = (Rnum + 1)*np.pi)"],"metadata":{"id":"qxF7_a4lg6kT","executionInfo":{"status":"aborted","timestamp":1750702342539,"user_tz":240,"elapsed":0,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32623,"status":"aborted","timestamp":1750702342540,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"1TPLz2zGk6O7"},"outputs":[],"source":["len(unverified_points)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32622,"status":"aborted","timestamp":1750702342540,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"961muK2uUlqZ"},"outputs":[],"source":["f_printer(verified_points.detach().to('cpu').numpy(), verified_radius.detach().to('cpu').numpy(), R = 4*np.pi)"]},{"cell_type":"code","source":["import torch\n","\n","def approximate_one_sided_lipschitz_controlled_linf(f, a, b, u_max, num_samples=100000, device=\"cpu\", seed=0, l = 0.5, m = 0.1):\n","    \"\"\"\n","    Approximate the one-sided Lipschitz constant over all x, y in domain and u in [-u_max, u_max].\n","\n","    Parameters:\n","    - f: function f(x, u), batched over x (N, 2) and scalar u\n","    - a, b: state space bounds\n","    - u_max: max control input\n","    - num_samples: number of (x, y, u) triples\n","    - device: torch device\n","    - seed: random seed\n","\n","    Returns:\n","    - approx_L: worst-case one-sided Lipschitz constant (L-infinity norm)\n","    \"\"\"\n","    torch.manual_seed(seed)\n","\n","    # Sample random states\n","    x = torch.empty((num_samples, 2), device=device).uniform_(-a, a)\n","    x[:, 1] = torch.empty(num_samples, device=device).uniform_(-b, b)\n","\n","    y = torch.empty((num_samples, 2), device=device).uniform_(-a, a)\n","    y[:, 1] = torch.empty(num_samples, device=device).uniform_(-b, b)\n","\n","    dx = x - y\n","    norm_inf2_dx = torch.max(torch.abs(dx), dim=1).values**2\n","\n","    # Sample shared control inputs\n","    u = torch.empty((num_samples,), device=device).uniform_(-u_max, u_max)\n","\n","    # Filter zero-distance cases\n","    nonzero = norm_inf2_dx > 1e-8\n","    x, y, dx, norm_inf2_dx, u = x[nonzero], y[nonzero], dx[nonzero], norm_inf2_dx[nonzero], u[nonzero]\n","\n","    # Evaluate vector field\n","    fx = f(x, u, m = m, l = l)\n","    fy = f(y, u, m = m, l = l)\n","    df = fx - fy\n","\n","    # Inner product numerator\n","    numerator = torch.sum(df * dx, dim=1)\n","    ratio = numerator / norm_inf2_dx\n","\n","    return ratio.max().item()\n","\n","def timecheck(l = 0.05, u_max = 0.3, m = 0.1):\n","  state = -torch.tensor([[torch.pi/12, 0]])\n","  start = state.clone()\n","  control = torch.tensor([u_max])\n","  i = 0\n","  while torch.abs(state[0][0]) >= torch.abs(start[0][0])-0.01:\n","    state = wrap_to_pi(state)\n","    delta = inverted_pendulum_2d_torch(state, -control, m = m, l = l, g = 9.81)\n","    state = state + delta * 0.05\n","    i+=1\n","  return i/20\n","\n","c = 3\n","m = 0.1\n","u_max = c * m\n","for l in [0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 1000, 10000]:\n","  L_inf_os = approximate_one_sided_lipschitz_controlled_linf(inverted_pendulum_2d_torch, a=torch.pi, b=5*torch.pi, u_max=u_max,  device=\"cuda\", num_samples=100000, l = l, m = m)\n","  print(f\"Approximate L-infinity one-sided Lipschitz constant: {L_inf_os:.4f}\")\n","  checktime = timecheck(l = l, u_max = u_max, m = m)\n","  #print(f\"Approximate Time: {checktime:.4f}\")\n","  print(f\"l: {l}, Total:  {checktime * L_inf_os:.4f}\")\n","\n"],"metadata":{"id":"eJ31eB9Z9IfR","executionInfo":{"status":"aborted","timestamp":1750702342541,"user_tz":240,"elapsed":32622,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state = torch.tensor([[torch.pi, 0]])\n","c = 3\n","m = 0.1\n","l = 10\n","control = torch.tensor([c * m])\n","i = 0\n","while state[0][1] > -0.01:\n","  state = wrap_to_pi(state)\n","  delta = inverted_pendulum_2d_torch(state, control, m = m, l = l, g = 9.81)\n","  state = state + delta * 0.05\n","  print(state)\n","  i+=1\n","while state[0][1] < 0.01:\n","  state = wrap_to_pi(state)\n","  delta = inverted_pendulum_2d_torch(state, -control, m = m, l = l, g = 9.81)\n","  state = state + delta * 0.05\n","  print(state)\n","  i+=1\n","while state[0][1] > -0.01:\n","  state = wrap_to_pi(state)\n","  delta = inverted_pendulum_2d_torch(state, control, m = m, l = l, g = 9.81)\n","  state = state + delta * 0.05\n","  print(state)\n","  i+=1\n","print(i)"],"metadata":{"id":"Ny93HJGL-pf4","executionInfo":{"status":"aborted","timestamp":1750702342553,"user_tz":240,"elapsed":32631,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state = -torch.tensor([[torch.pi/10, 0]])\n","s = -torch.tensor([[torch.pi/10, 0]])\n","c = 3\n","m = 0.1\n","l = 10\n","control = torch.tensor([c * m])\n","i = 0\n","while state[0][0] < 0:\n","  state = wrap_to_pi(state)\n","  delta = inverted_pendulum_2d_torch(state, control, m = m, l = l, g = 9.81)\n","  state = state + delta * 0.05\n","  print(state)\n","  i+=1"],"metadata":{"id":"uyP4dv_y-3KE","executionInfo":{"status":"aborted","timestamp":1750702342554,"user_tz":240,"elapsed":32631,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32631,"status":"aborted","timestamp":1750702342556,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"XrGwFpQJms9L"},"outputs":[],"source":["import torch\n","sentinel = 1337\n","\n","spot = []\n","spot.append(torch.tensor([[torch.pi - 0.001, 0.0]], device = 'cuda'))\n","env1 = PendulumEnv(num_envs = 1, dimension = 2, max_radius=np.pi/2, max_speed = 2, rate = 0.05, field_function = simplified_pendulum_derivatives)\n","env1.reset(spot[-1])\n","controls = []\n","for i in range(100):\n","    s = wrap_to_pi(spot[-1])\n","    print(s)\n","    idx = find_hypercubes(s, verified_points, verified_radius)\n","    print(idx)\n","    if idx != -1:\n","      print('here')\n","      action = verified_controls[idx]\n","      t = verified_indices[idx]\n","      action = pad_tensor_1d(action[0], t.item()).unsqueeze(0)\n","      controls.append(action)\n","      traj = env1.trajectories(action)\n","      traj = wrap_to_pi(traj)\n","      print(traj[:, :t])\n","      #reward = reward_func(traj[:, :t+1, :], action[:, :t])\n","      #rewards.extend(reward)\n","      i = i + t\n","      spot.append(traj[:, -1, :])\n","      env1.reset(spot[-1])\n","\n","\n","spot = np.array(spot)\n","plt.plot(controls)\n","plt.plot(spot[:,0])\n","plt.plot(spot[:,1])\n","plt.legend(['control', 'location', 'velocity'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32631,"status":"aborted","timestamp":1750702342557,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"6pUDiDL6TnLX"},"outputs":[],"source":["locp = overall_points.view(len(overall_points), len(maxdists)-1, 2)[:,:,0].cpu().detach().numpy()\n","speedp = overall_points.view(len(overall_points), len(maxdists)-1, 2)[:,:,1].cpu().detach().numpy()\n","\n","\n","norms = torch.linalg.norm(trajectories, ord = float('inf'), dim = 2).cpu().detach().numpy()\n","\n","loc = trajectories[:,:,0].cpu().detach().numpy()\n","speed = trajectories[:,:,1].cpu().detach().numpy()\n","control = end_controls.cpu().detach().numpy()\n","\n","a1 = 3\n","a2 = 55\n","a3 = 89\n","a4 = 32\n","a5 = 84\n","a6 = 41\n","\n","for i in range(len(norms)):\n","  plt.plot(np.array(range(len(locp[0]))), locp[i])\n","\n","plt.xlabel('Time (s)')\n","plt.ylabel('Location')\n","plt.title('Location vs Time (Ends)')\n","plt.show()\n","\n","\n","for i in range(len(norms)):\n","  l = []\n","  for j in range(len(loc[i])):\n","    if j == 0 or control[i][j-1] != sentinel:\n","      l.append(loc[i][j])\n","  plt.plot(np.array(range(len(l)))/10, l)\n","\n","plt.xlabel('Time (s)')\n","plt.ylabel('Location')\n","plt.title('Location vs Time (Full)')\n","plt.show()\n","\n","\n","for i in range(len(speed)):\n","  s = []\n","  for j in range(len(speed[i])):\n","    if j != 0 and control[i][j - 1] is sentinel:\n","      print(control[i][j-1])\n","    if j == 0 or control[i][j-1] != sentinel:\n","      s.append(speed[i][j])\n","  plt.plot(np.array(range(len(s)))/10, s)\n","\n","plt.xlabel('Time (s)')\n","plt.ylabel('Velocity')\n","plt.title('Velocity vs Time (Full)')\n","plt.show()\n","\n","\n","for i in range(len(norms)):\n","  plt.plot(loc[i], speed[i])\n","\n","\n","plt.xlabel('Location')\n","plt.ylabel('Speed')\n","plt.title('Phase Portrait')\n","plt.show()\n","\n","\n","for i in range(len(norms)):\n","  c = []\n","  for j in range(len(control[i])):\n","    if control[i][j] != sentinel:\n","      c.append(control[i][j])\n","  plt.plot(np.array(range(len(c)))/10, c)\n","\n","plt.xlabel('Time (s)')\n","plt.ylabel('Control')\n","plt.title('Control vs Time (Full)')\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32629,"status":"aborted","timestamp":1750702342557,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"OYJZDUp9ZtWa"},"outputs":[],"source":["import time\n","import matplotlib.pyplot as plt\n","\n","\n","def reward_func(state, control):\n","  return -(state[0, 1:, 0]**2 +0.1*state[0, 1:, 1]**2 + 0.001*control**2)\n","\n","def reward_func_batched(states, controls, taus):\n","    \"\"\"\n","    Compute rewards for a batch of environments up to time t_i for each i.\n","\n","    Args:\n","        states: Tensor of shape (batch, T+1, 2)\n","        controls: Tensor of shape (batch, T)\n","        taus: Tensor of shape (batch,) with integers for each env\n","\n","    Returns:\n","        rewards: Tensor of shape (batch,)\n","    \"\"\"\n","    B, T_plus_1, D = states.shape\n","    T = T_plus_1 - 1\n","\n","    time_indices = torch.arange(T, device=states.device).unsqueeze(0)  # shape (1, T)\n","    taus_expanded = taus.unsqueeze(1)  # shape (B, 1)\n","    mask = time_indices < taus_expanded  # shape (B, T)\n","\n","    pos = states[:, 1:, 0]\n","    vel = states[:, 1:, 1]\n","    control = controls\n","\n","    total_cost = pos**2 + 0.1*vel**2 + 0.001 * control**2  # shape (B, T)\n","    total_cost = total_cost * mask  # mask entries beyond t_i\n","    reward = -total_cost.sum(dim=1)\n","\n","    # # DEBUG PRINTS\n","    # print(\"taus:\", taus)\n","    # print(\"avg theta:\", (pos*mask).abs().mean().item())\n","    # print(\"avg omega:\", (vel*mask).abs().mean().item())\n","    # print(\"avg control:\", (control*mask).abs().mean().item())\n","    # print(\"reward per env:\", reward)\n","\n","    return reward  # final shape: (B,)\n","\n","\n","def find_step(obs, centers, radii, controls, indices):\n","\n","  index = find_hypercube(obs[0], centers, radii)\n","  t = indices[index]\n","  control = controls[index]\n","  control = pad_tensor_1d(control, t).unsqueeze(0)\n","\n","  return control, t\n","\n","def find_steps(obs, centers, radii, controls, indices):\n","\n","  index = find_hypercubes(obs, centers, radii)\n","  t = indices[index]\n","  control = controls[index]\n","  control = pad_tensor_rows_1d(control, t)\n","\n","  return control, t\n","\n","def auc_pytorch(centers, radii, controls, indices, episodes=10, render=False):\n","    env = PendulumEnv(num_envs = 1, dimension = 2, max_radius=np.pi/2, max_speed = 2, rate = 0.05)\n","    aucs = []\n","    its = []\n","    times = []\n","    for ep in range(episodes):\n","        it = 0\n","        start = time.time()\n","        if ep % 10 == 0:\n","          print(ep)\n","        obs = env.sample_points(1)\n","        env.reset(obs)\n","        rewards = []\n","        i = 0\n","        while i < 200:\n","            action, t = find_step(obs, centers, radii, controls, indices)\n","            traj = env.trajectories(action)\n","            traj = wrap_to_pi(traj)\n","            reward = reward_func(traj[:, :t+1, :], action[:, :t])\n","            rewards.extend(reward)\n","            i = i + t + 1\n","            if render:\n","                env.render()\n","            obs = traj[:, -1, :]\n","            env.reset(obs)\n","            it += 1\n","        its.append(it)\n","\n","        rewards = torch.cat(rewards).tolist()\n","        auc = np.sum(rewards)\n","        aucs.append(auc)\n","        times.append(time.time() - start)\n","\n","    #env.close()\n","    mean_auc = np.mean(aucs)\n","    print(f\"Mean AUC over {episodes} randomly-initialized episodes: {mean_auc:.2f}\")\n","\n","    # Plot\n","    plt.plot(aucs, marker='o')\n","    plt.title(\"Episode-wise AUC (Random Init)\")\n","    plt.xlabel(\"Episode\")\n","    plt.ylabel(\"Total Reward (AUC)\")\n","    plt.grid(True)\n","    plt.show()\n","\n","    return aucs, times, its\n","\n","\n","\n","def auc_pytorch_parallel(centers, radii, controls, indices, episodes=10, seed=None, function = simplified_pendulum_derivatives):\n","    env = PendulumEnv(num_envs = episodes, dimension = 2, max_radius=np.pi, max_speed = 2, rate = 0.05, field_function = function)\n","    aucs = []\n","    its = []\n","    times = []\n","    reward = torch.zeros(episodes).to('cuda')\n","    if True:\n","        it = torch.zeros(episodes).to('cuda')\n","        start = time.time()\n","        obs = env.sample_points(episodes)\n","        if episodes == 1:\n","          if seed is not None:\n","            obs = torch.tensor([seed]).to('cuda')\n","          trajectory = torch.cat((obs, torch.Tensor([[0]]).to('cuda')), dim = 1)\n","        env.reset(obs)\n","        i = torch.zeros(episodes).to('cuda')\n","        while torch.any(i < 200):\n","            mask = i < 200\n","            env = PendulumEnv(num_envs = torch.sum(mask).item(), dimension = 2, max_radius=np.pi/2, max_speed = 2, rate = 0.05, field_function = function)\n","            env.reset(obs[mask])\n","            action, t = find_steps(obs[mask], centers, radii, controls, indices)\n","            traj = env.trajectories(action)\n","            traj = wrap_to_pi(traj)\n","            if episodes == 1:\n","              snippet =  torch.cat((traj[0, 1:t+1, :], action[:, :t].transpose(0,1)), dim = 1)\n","              trajectory = torch.cat((trajectory, snippet), dim=0)\n","            reward[mask] += reward_func_batched(traj, action, t).to('cuda')\n","            i[mask] = i[mask] + t\n","            obs[mask] = traj[:, -1, :]\n","            it[mask] += 1\n","        times.append(time.time() - start)\n","\n","    if episodes == 1:\n","      plt.plot(trajectory.cpu().detach().numpy())\n","      plt.title(\"Trajectory\")\n","      plt.xlabel(\"Time\")\n","      plt.ylabel(\"Location\")\n","      plt.grid(True)\n","      legend = [\"theta\", \"thetadot\", \"u\"]\n","      plt.legend(legend)\n","      plt.show()\n","\n","\n","\n","      plt.plot(trajectory[:, 0].cpu().detach().numpy(), trajectory[:, 1].cpu().detach().numpy())\n","      plt.title(\"Phase Portrait\")\n","      plt.xlabel(\"Location\")\n","      plt.ylabel(\"Velocity\")\n","      plt.grid(True)\n","      plt.show()\n","    aucs = (reward).detach().cpu().numpy()\n","    #env.close()\n","    mean_auc = np.mean(aucs)\n","    print(f\"Mean AUC over {episodes} randomly-initialized episodes: {mean_auc:.2f}\")\n","\n","    # Plot\n","    plt.plot(aucs, marker='o')\n","    plt.title(\"Episode-wise AUC (Random Init)\")\n","    plt.xlabel(\"Episode\")\n","    plt.ylabel(\"Total Reward (AUC)\")\n","    plt.grid(True)\n","    plt.show()\n","\n","    return aucs, times, it\n","\n","\n","auc, times, its = auc_pytorch_parallel(verified_points, verified_radius, verified_controls, verified_indices, episodes = 1, seed = [torch.pi/4, 0])\n","#auc, times, its = auc_pytorch_parallel(verified_points, verified_radius, verified_controls, verified_indices, episodes = 1)\n","\n","print(times)\n","print(torch.mean(its))\n","\n","\n","# auc, times, its = auc_pytorch_parallel(verified_points_alt1, verified_radius_alt1, verified_controls_alt1, verified_indices_alt1, episodes = 1, seed = [torch.pi, 0])\n","# print(times)\n","# print(torch.mean(its))\n","\n"]},{"cell_type":"code","source":["simplified_pendulum_derivatives(torch.tensor([[-np.pi/2, 0]]), torch.tensor([0]))"],"metadata":{"id":"JiCYg-5OP68p","executionInfo":{"status":"aborted","timestamp":1750702342558,"user_tz":240,"elapsed":32629,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32628,"status":"aborted","timestamp":1750702342558,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"0Lytbrq9CJEN"},"outputs":[],"source":["import pickle\n","import torch\n","\n","\n","with open('timesave_131400.pkl', 'rb') as file:\n","        ll = pickle.load(file)\n","\n","verified_points = ll[0]\n","verified_radius = ll[1]\n","verified_controls = ll[2]\n","verified_indices = ll[4]\n","verified_alphas = ll[3]\n","verified_indices[0] = 1\n","\n","verified_points, verified_radius, verified_controls, verified_alphas, verified_indices, points, radius, controls, splits, unverified_points, unverified_radius = ll\n","\n","f_printer(verified_points.detach().to('cpu').numpy(), verified_radius.detach().to('cpu').numpy(), R = 4*np.pi)"]},{"cell_type":"code","source":["f_printer(verified_points.detach().to('cpu').numpy(), verified_radius.detach().to('cpu').numpy(), R = 4*np.pi)"],"metadata":{"id":"bhvhI0ncKh3q","executionInfo":{"status":"aborted","timestamp":1750702342559,"user_tz":240,"elapsed":32628,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(unverified_points)"],"metadata":{"id":"P7BdRH_VnyAD","executionInfo":{"status":"aborted","timestamp":1750702342559,"user_tz":240,"elapsed":32626,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32635,"status":"aborted","timestamp":1750702342569,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"-E8bn1gGZyr0"},"outputs":[],"source":["import torch\n","\n","def find_hypercube(point, centers, radii):\n","    \"\"\"\n","    Returns the index of the first hypercube that contains `point`, or the index of the closest center if none do.\n","\n","    Args:\n","        point: Tensor of shape (n,)\n","        centers: Tensor of shape (m, n)\n","        radii: Tensor of shape (m,) or (m, n)\n","\n","    Returns:\n","        int: Index of matching or closest hypercube\n","    \"\"\"\n","    point = point.flatten()\n","    #point = wrap_to_pi(point)\n","    m, n = centers.shape\n","\n","    if radii.dim() == 0:\n","        radii = radii.view(1, 1)\n","    elif radii.dim() == 1:\n","        radii = radii.unsqueeze(1)\n","    radii = radii.expand(m, n)\n","\n","    lower_bounds = centers - radii\n","    upper_bounds = centers + radii\n","\n","    contained = (point >= lower_bounds) & (point <= upper_bounds)\n","    inside_mask = contained.all(dim=1)\n","\n","    if inside_mask.any():\n","        return torch.nonzero(inside_mask, as_tuple=False)[0].item()\n","    else:\n","        dists = torch.norm(centers - point, dim=1)\n","        print('AHHHH')\n","        return torch.argmin(dists).item()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32635,"status":"aborted","timestamp":1750702342570,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"zRgy7J3YcekY"},"outputs":[],"source":["import torch\n","\n","def pad_tensor_1d(tensor, k, sentinel=1337):\n","    \"\"\"\n","    Replace the last (n - k) elements of a 1D tensor with a sentinel value.\n","\n","    Args:\n","        tensor (torch.Tensor): Input tensor of shape (n,).\n","        k (int): Number of elements to keep (must be 0 <= k <= n).\n","        sentinel (float or int): Value to insert in padded positions.\n","\n","    Returns:\n","        torch.Tensor: Modified tensor with sentinel padding applied.\n","    \"\"\"\n","    n = tensor.shape[0]\n","    assert 0 <= k <= n, f\"k must be between 0 and {n}, but got {k}\"\n","\n","    mask = torch.arange(n, device=tensor.device) < k\n","    return torch.where(mask, tensor, torch.tensor(sentinel, dtype=tensor.dtype, device=tensor.device))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32634,"status":"aborted","timestamp":1750702342570,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"6zIwimVPL0I4"},"outputs":[],"source":["import time\n","import numpy as np\n","device = 'cuda'\n","ftimes = []\n","for n in [1,10,100,1000,10000]:\n","  t = []\n","  for i in range(10000):\n","    points = 2*torch.pi*(torch.rand(n, 2)-0.5).to('cuda')\n","    start = time.time()\n","    f1 = find_hypercubes(points, verified_points, verified_radius)\n","    t.append(time.time()-start)\n","  ftimes.append(t)\n","print([np.mean(t) for t in ftimes])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32634,"status":"aborted","timestamp":1750702342571,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"EFp7VHmGdm8_"},"outputs":[],"source":["import time\n","import numpy as np\n","device = 'cuda'\n","ftimes = []\n","for n in [1,10,100,1000,10000]:\n","  c = verified_points[:n, :]\n","  r = verified_radius[:n]\n","  t = []\n","  for i in range(10000):\n","    points = 2*torch.pi*(torch.rand(1000, 2)-0.5).to('cuda')\n","    start = time.time()\n","    f1 = find_hypercubes(points, c, r)\n","    t.append(time.time()-start)\n","  ftimes.append(t)\n","print([np.mean(t) for t in ftimes])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32632,"status":"aborted","timestamp":1750702342571,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"bYWQxONOpBRy"},"outputs":[],"source":["#!pip install rtree\n","import torch\n","import numpy as np\n","from rtree import index\n","import time\n","n = 1000\n","\n","# Sample tensor points\n","points = torch.tensor([[1.0, 2.0], [2.0, 3.0]])\n","points = 2*torch.pi*(torch.rand(n, 2)-0.5).to('cuda')\n","\n","# Convert to NumPy for R-tree\n","points_np = points.cpu().numpy()\n","v_points_np = verified_points.cpu().numpy()\n","radius_np = verified_radius.cpu().numpy()\n","\n","start = time.time()\n","# Build R-tree index\n","p = index.Property()\n","idx = index.Index(properties=p)\n","for i, point in enumerate(v_points_np):\n","    x, y = point\n","    r = radius_np[i]\n","    idx.insert(i, (x-r, y-r, x+r, y+r))  # Rtree requires bounding boxes\n","\n","for i in range(n):\n","  point = points_np[i]\n","  matches = list(idx.intersection((point[0], point[1], point[0], point[1])))\n","  #print(len(matches))\n","print(time.time()-start)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32632,"status":"aborted","timestamp":1750702342572,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"eF5V2ozds8ZX"},"outputs":[],"source":["import torch\n","import numpy as np\n","#from rtree import index\n","import time\n","n = 80000\n","\n","# Sample tensor points\n","points = torch.tensor([[1.0, 2.0], [2.0, 3.0]])\n","points = 2*torch.pi*(torch.rand(n, 2)-0.5).to('cuda')\n","\n","start = time.time()\n","\n","find_hypercube_intersections_wrap(verified_points, verified_radius, points, 0.1+torch.zeros(points.shape[0], device = 'cuda'))\n","\n","print(time.time()-start)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32632,"status":"aborted","timestamp":1750702342573,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"cijjqNuu_Snq"},"outputs":[],"source":["import gc\n","\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32633,"status":"aborted","timestamp":1750702342575,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"rM8wl9KQF7K6"},"outputs":[],"source":["def count_hypercube_intersections(centers, radii, query_centers, query_radii):\n","    \"\"\"\n","    Count how many hypercubes intersect each query hypercube.\n","    centers: (N, D)\n","    radii: (N, 1) or (N, D)\n","    query_centers: (M, D)\n","    query_radii: (M, 1) or (M, D)\n","    \"\"\"\n","    N, D = centers.shape\n","    M, _ = query_centers.shape\n","\n","    # Make sure radii are (N, D)\n","    if radii.ndim == 1:\n","        radii = radii.unsqueeze(1).expand(N, D)\n","    elif radii.shape[1] == 1:\n","        radii = radii.expand(N, D)\n","\n","    if query_radii.ndim == 1:\n","        query_radii = query_radii.unsqueeze(1).expand(M, D)\n","    elif query_radii.shape[1] == 1:\n","        query_radii = query_radii.expand(M, D)\n","\n","    # Expand for broadcasting\n","    centers = centers.unsqueeze(0)             # (1, N, D)\n","    radii = radii.unsqueeze(0)                 # (1, N, D)\n","    query_centers = query_centers.unsqueeze(1) # (M, 1, D)\n","    query_radii = query_radii.unsqueeze(1)     # (M, 1, D)\n","\n","    # Compute overlaps\n","    dist = torch.abs(centers - query_centers)      # (M, N, D)\n","    threshold = radii + query_radii                # (M, N, D)\n","    overlap = dist <= threshold                    # (M, N, D)\n","\n","    intersects = overlap.all(dim=-1)               # (M, N)\n","    return intersects.sum(dim=1)                   # (M,)\n","\n","\n","start = time.time()\n","c = count_hypercube_intersections(verified_points, verified_radius, points, torch.zeros(points.shape[0], device = 'cuda'))\n","h = find_hypercubes(points, verified_points, verified_radius)\n","print((h == -1) == (c == 0))\n","print(time.time()-start)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32634,"status":"aborted","timestamp":1750702342577,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"OemRpzsrM01H"},"outputs":[],"source":["#!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n","\n","from torch_scatter import scatter\n","\n","def find_hypercube_intersections(centers, radii, query_centers, query_radii):\n","    \"\"\"\n","    For each query hypercube, return a list of tensors with the indices of intersecting hypercubes.\n","\n","    Inputs:\n","    - centers: (N, D)\n","    - radii: (N,) or (N, D)\n","    - query_centers: (M, D)\n","    - query_radii: (M,) or (M, D)\n","\n","    Returns:\n","    - List of M tensors, each containing indices of intersecting hypercubes.\n","    \"\"\"\n","    N, D = centers.shape\n","    M = query_centers.shape[0]\n","\n","    if radii.ndim == 1:\n","        radii = radii.unsqueeze(1).expand(N, D)\n","    elif radii.shape[1] == 1:\n","        radii = radii.expand(N, D)\n","\n","    if query_radii.ndim == 1:\n","        query_radii = query_radii.unsqueeze(1).expand(M, D)\n","    elif query_radii.shape[1] == 1:\n","        query_radii = query_radii.expand(M, D)\n","\n","    # Broadcasting\n","    qc = query_centers.unsqueeze(1)  # (M, 1, D)\n","    qr = query_radii.unsqueeze(1)    # (M, 1, D)\n","    c = centers.unsqueeze(0)         # (1, N, D)\n","    r = radii.unsqueeze(0)           # (1, N, D)\n","\n","    dist = torch.abs(c - qc)         # (M, N, D)\n","    threshold = r + qr               # (M, N, D)\n","    mask = (dist <= threshold).all(dim=-1)  # (M, N)\n","\n","    # Fast extraction of intersecting indices\n","    query_idx, box_idx = torch.nonzero(mask, as_tuple=True)  # (K,), (K,)\n","\n","    # Find how many boxes per query\n","    counts = torch.bincount(query_idx, minlength=M)\n","\n","    # Split `box_idx` at cumulative sums of counts\n","    splits = counts.cumsum(0)\n","    splits = torch.cat([splits.new_zeros(1), splits])\n","\n","    return [box_idx[splits[i]:splits[i+1]] for i in range(M)]\n","\n","\n","\n","\n","start = time.time()\n","find_hypercube_intersections(verified_points, verified_radius, points, 0.1+torch.zeros(points.shape[0], device = 'cuda'))\n","print(time.time()-start)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32640,"status":"aborted","timestamp":1750702342585,"user":{"displayName":"Roy Siegelmann","userId":"17422578296280154477"},"user_tz":240},"id":"xFaDoDPP9ZlU"},"outputs":[],"source":["import torch\n","\n","def find_hypercubes_old(points, centers, radii):\n","    \"\"\"\n","    Vectorized version: returns the first matching hypercube index for each point,\n","    or -1 if none are found.\n","    \"\"\"\n","\n","    points = wrap_to_pi(points)\n","    k, n = points.shape\n","    m, _ = centers.shape\n","\n","    if radii.dim() == 1:\n","        radii = radii.unsqueeze(1)\n","    radii = radii.expand(m, n)\n","\n","    lower_bounds = centers - radii  # (m, n)\n","    upper_bounds = centers + radii  # (m, n)\n","\n","    # (k, 1, n) vs (1, m, n) → (k, m, n)\n","    points = points.unsqueeze(1)\n","    contained = (points >= lower_bounds) & (points <= upper_bounds)\n","    inside_mask = contained.all(dim=2)  # (k, m)\n","\n","    # Set all False to large positive index (m), then take min index along dim=1\n","    masked_indices = torch.where(inside_mask, torch.arange(m, device=points.device), m)\n","    min_indices = masked_indices.min(dim=1).values\n","\n","    # Set to -1 if no hypercube matched (i.e. if index == m)\n","    result = torch.where(min_indices == m, torch.full_like(min_indices, -1), min_indices)\n","\n","\n","    return result\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1RtsAMXgPin4bKpCIzMTGrGlrRwXE_kSC","timestamp":1749071190703},{"file_id":"1GX25Bf7TJOplFd1swJg5tHbSidQy54m6","timestamp":1749046736187}],"authorship_tag":"ABX9TyNz//5cGwz2esBeFlggop0l"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}